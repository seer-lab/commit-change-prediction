\chapter{Experiments}
\label{chap:experiments}

Experiments were conducted in order to provide some validation for the proposed approach. The goal of these experiments is to test the whether the historical commit data from an \gls{oss} project can be used towards predicting future changes within the project. These experiments are based on the approach outlined in the previous chapter, \autoref{chap:prediction}. The experiment was conducted through measuring the performance of each core factors varied in isolation. Specifically the \gls{swr}, the feature set and categorization balancing using \gls{os}. These three factors are explored for both machine learning algorithms.

\section{Experimental Project Data}
\label{sec:experimental_project_data}

%One thing that should be noted for the experiments that were run. Since all of the data was known before the model was even created artificial cut off dates were created to allow for the feature set to be tested as to their effect on the model. A test project, acra (developed by the user ACRA), was chosen to develop the method on. 

%The project data was extracted from GitHub from when the project was initially committed to GitHub (2010-04-18 15:52:18-04) till the cut off day of extraction (2015-06-05 09:02:56-04). The cut off day was the day the data was extracted from GitHub and after of which the data analysis was initiated.

The complete list of projects that were experimented on are found in \autoref{tab:project_summary}. Data from each project was collected from the creation date for the project till the date the data was collected on. The number of commits excludes any commit that lacked a change to a file containing Java code. Since the primary interest was to parse Java code, files containing Java code were used while all other files are ignored. These measures provide a more accurate description of the project in terms of the analysis and predictions made on it. Secondly, the number of developers does not map effectively to what git uses as committers and authors. Instead, the number of developers includes all individuals (removing duplicates) who committed or authored commits to the current project.

% TODO consider placing this into introduction
\begin{table}[!hbp]
%\begin{minipage}{\textwidth}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Owner & Project & Start Date & End Date & \# of & \# of \\
         & & & & Commits & Developers \\
        \hline
        ACRA & acra & 2010-04-18 & 2015-06-05 & 404 & 32 \\
        arquillian & arquillian-core & 2009-11-13 & 2016-03-16 & 473 & 49 \\
        google & blockly-android & 2015-07-23 & 2016-06-23 & 691 & 8 \\
        openzipkin & brave & 2013-04-07 & 2016-06-21 & 337 & 32 \\
        gabrielemariotti & cardslib & 2013-09-20 & 2015-05-12 & 327 & 13 \\
        square & dagger & 2012-06-25 & 2016-01-30 & 496 & 38 \\
        deeplearning4j & deeplearning4j & 2013-11-27 & 2016-02-13 & 3523 & 61 \\
        facebook & fresco & 2015-03-26 & 2015-10-30 & 313 & 45 \\
        Netflix & governator & 2012-03-18 & 2016-06-23 & 621 & 31 \\
        greenrobot & greenDAO & 2011-07-28 & 2016-05-23 & 415 & 4 \\
        kevinsawicki & http-request & 2011-10-21 & 2015-01-21 & 273 & 14 \\
        koush & ion & 2013-05-22 & 2016-06-14 & 520 & 29 \\
        skylot & jadx & 2013-03-18 & 2016-03-27 & 480 & 11 \\
        mapstruct & mapstruct & 2012-05-28 & 2016-06-15 & 604 & 22 \\
        Atmosphere & nettosphere & 2012-02-09 & 2016-04-11 & 336 & 12 \\
        johncarl81 & parceler & 2013-07-03 & 2016-06-22 & 228 & 12 \\
        orfjackal & retrolambda & 2013-07-20 & 2016-04-30 & 275 & 11 \\
        amlcurran & ShowcaseView & 2012-08-14 & 2016-05-30 & 332 & 39 \\
        haifengl & smile & 2014-11-20 & 2016-06-24 & 237 & 14 \\
        perwendel & spark & 2011-05-05 & 2016-06-19 & 551 & 86 \\
        apache & storm & 2011-09-16 & 2015-12-28 & 2445 & 260 \\
        prestodb & tempto & 2015-03-06 & 2016-06-20 & 298 & 19 \\
        gridgain & yardstick & 2014-04-11 & 2015-10-12 & 213 & 12 \\
        \hline
    \end{tabular}
\end{center}
\caption{Experiment projects}
\label{tab:project_summary}
%\end{minipage}
\end{table}

Given the large number of project experimented on a categorization system was necessary to allow for grouping of projects. Four project measures were selected for comparing the projects and are outlined in \autoref{tab:project_size_summary_info}. The measures are: project length in years, project size in number of methods, number of developers and the rate of commits made in commits per year. The project length represents the number of years the project has been under development for. The size of the project is measured in the number of method signatures within the project since created. The number of developers is tallied from the beginning of the project for this measure. Finally, the rate of commits is the number of commits contributed to the project per year. A yearly rate of commits was sufficient since the majority of the projects had more than one year of development history.


\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        project length & project size & \# devs & commit rate \\
        \hline
        \hline
        short ($t < 1$) & small ($m < 2000$) & small ($d < 30$) & low rate ($r < 100$) \\
        \hline
        medium & medium & medium & medium \\
        ($1 \leq t < 3$) & ($2000 \leq m < 10000$) & ($30 \leq d < 100$) & ($100 \leq r < 300$) \\
        \hline
        long ($t \geq 3$) & large ($m \geq 10000$) & large ($d \geq 100$) & high rate ($300 \leq r < 600$) \\
        \hline
        & & & very high rate ($r \geq 600$) \\
        \hline
    \end{tabular}
\end{center}
\caption{Experiment project summary}
\label{tab:project_size_summary_info}
\end{table}

% \begin{table}
% \begin{center}
%     \begin{tabular}{|c|c|c|c|c|}
%         \hline
%         project length & short ($t < 12 m$) & medium ($12 m \leq t < 3 y$) & long ($t \geq 3 y$) & \\
%         project size & small ($m < 2000$) & medium ($2000 \leq m < 10000$) & large ($m \geq 10000$) & \\
%         \# devs & small ($d < 30$) & medium ($30 \leq d < 100$) & $d \geq 100$ & \\
%         commit rate & low rate ($r < 100$) & medium ($100 \leq r < 300$) & high rate ($300 \leq r < 600$) & very high rate ($r \geq 600$) \\
%         \hline
%     \end{tabular}
% \end{center}
% \caption{Experiment project summary}
% \label{tab:project_size_summary_info}
% \end{table}

Using the classifications outline in \autoref{tab:project_size_summary_info}, the projects are grouped and organized with similar projects. In \autoref{tab:project_size_summary} the projects are sorted by their classification and have dividing lines around similar projects. For example four projects; http-request, nettosphere, parceler and retrolambda are all classified in the same group. Some projects like ion or storm are not grouped in with another project and thus are in a group of their own.

% TODO place in a better location
% TODO talk about the table
\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        name & project length & project size & \# devs & commit rate \\
        \hline
        \hline
        yardstick & medium & small & small & medium \\
        \hline
        tempto & medium & medium & small & medium \\
        \hline
        blockly-android & medium & medium & small & high \\
        \hline
        fresco & medium & medium & medium & high \\
        \hline
        http-request & long & small & small & low \\
        nettosphere & long & small & small & low \\
        parceler & long & small & small & low \\
        retrolambda & long & small & small & low \\
        \hline
        ion & long & small & small & medium \\
        \hline
        acra & long & small & medium & low \\
        dagger & long & small & medium & low \\
        ShowcaseView & long & small & medium & low \\
        \hline
        greenDAO & long & medium & small & low \\
        smile & long & medium & small & low \\
        \hline
        cardslib & long & medium & small & medium \\
        jadx & long & medium & small & medium \\        
        mapstruct & long & medium & small & medium \\
        \hline
        arquillian-core & long & medium & medium & low \\
        brave & long & medium & medium & low \\
        spark & long & medium & medium & low \\
        \hline    
        
        governator & long & medium & medium & medium \\
        \hline
        deeplearning4j & long & large & medium & very high \\
        \hline
        storm & long & large & large & high \\
        \hline
    \end{tabular}
\end{center}
\caption{Experiment project summary}
\label{tab:project_size_summary}
\end{table}

%TODO place the tables in better positions.
Each of the projects are selected from GitHub using the list of Java projects with a large amount of development. \gls{oss} projects were targeted to simplify any usage concerns. Specifically, \gls{oss} projects are open and freely available immediately and can be discussed without restriction. Therefore in order to be selected the program had to clearly use an \gls{oss} license. Secondly, the project also needed to have at least a 6 months worth of development and at least 300 commits to provide a large enough dataset to analyze. An effort was also made to pick projects of different sizes to provide better tests of various conditions.

\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Project & \# of Methods & \# of Methods & Avg \# of & Avg \# of Methods \\
         & & Changes & Commits / Year & Change / Commit \\
        \hline
        acra & 1309 & 3605 & 67.33 & 9.51 \\
        arquillian-core & 5563 & 6657 & 59.13 & 15.2 \\
        blockly-android & 3608 & 9679 & 345.5 & 14.82 \\
        brave & 4204 & 7823 & 84.25 & 26.98 \\
        cardslib & 3940 & 5122 & 109.0 & 16.68 \\
        dagger & 1827 & 6314 & 99.2 & 13.7 \\
        deeplearning4j & 29896 & 82198 & 880.75 & 24.33 \\
        fresco & 3463 & 4139 & 313.0 & 14.73 \\
        governator & 4229 & 10946 & 124.2 & 19.04 \\
        greenDAO & 4089 & 8625 & 69.17 & 21.84 \\
        http-request & 726 & 1740 & 54.6 & 6.72 \\
        ion & 1678 & 4347 & 130.0 & 8.82 \\
        jadx & 6012 & 9322 & 120.0 & 19.63 \\
        mapstruct & 7885 & 10185 & 120.8 & 19.04 \\
        nettosphere & 1112 & 2857 & 67.2 & 9.01 \\
        parceler & 1619 & 3076 & 57.0 & 14.72 \\
        retrolambda & 1111 & 2588 & 68.75 & 9.95 \\
        ShowcaseView & 927 & 2672 & 66.4 & 8.62 \\
        smile & 3885 & 3879 & 79.0 & 18.47 \\
        spark & 3117 & 9154 & 91.83 & 18.27 \\
        storm & 14599 & 50037 & 489.0 & 24.03 \\
        tempto & 2422 & 3386 & 149.0 & 11.96 \\
        yardstick & 512 & 1216 & 106.5 & 6.37 \\
        \hline
    \end{tabular}
\end{center}
\caption{Project Change Statistics}
\label{tab:project_stats}
\end{table}

In order to get a more detailed understand of the selected projects, numerous measures were taken. These measures also allow for each projects to be compared to each other in terms of the development of each of the projects. For example the size of the project is represented through several measures including: number of commits, methods and developers. Several averages are calculated to help establish how the development occurred within a project during the development. A few examples of average measurements are the number of commits per year, changes per method.

% TODO complete list of projects
\begin{enumerate}
\item \textbf{acra}\footnote{\url{https://github.com/ACRA/acra}} is an Android bug logging tool used with Android applications to capture information related to bugs or crashes. The information is sent to the developers to help them address the issues that their clients encounter while using there application.
\item \textbf{arquillian-core}\footnote{\url{https://github.com/arquillian/arquillian-core}} is a platform for creating automated integration, functional and acceptance tests for Java middleware products. 
\item \textbf{blockly-android}\footnote{\url{https://github.com/google/blockly-android}} provides a native implementation of the blockly library for drag and drop development on Android. 
\item \textbf{brave}\footnote{\url{https://github.com/openzipkin/brave}} provides a Java distributed tracing tool for troubleshooting latency problems and is compatible with Zipkin.
\item \textbf{cardslib}\footnote{\url{https://github.com/gabrielemariotti/cardslib}} is an Android library for creating UI Cards in an Android application.
\item \textbf{dagger}\footnote{\url{https://github.com/square/dagger}} from square is a Java application used to satisfy dependencies for classes to replace the factory model of development.
\item \textbf{deeplearning4j}\footnote{\url{https://github.com/deeplearning4j/deeplearning4j}} is a distributed neural network library that integrates Hadoop and Spark. This application is the largest of the all the projects and provides a large wealth of data to analyze.
\item \textbf{fresco}\footnote{\url{https://github.com/facebook/fresco}} from facebook is the smallest project with the shortest development period. This project provides a library for using images on Android to attempt to solve limited memory issues with mobile devices.
\item \textbf{governator}\footnote{\url{https://github.com/Netflix/governator}} is a library of extensions and utilities that enhances Google's Guice to provide injector life-cycle and object life-cycle.
\item \textbf{greenDAO}\footnote{\url{https://github.com/greenrobot/greenDAO}} provides an Android based light and fast object relational mapping to SQLite database entries.
\item \textbf{http-request}\footnote{\url{https://github.com/kevinsawicki/http-request}} is a library accessing the \textit{httpURLConnection} to make requests and then access the response.
\item \textbf{ion}\footnote{\url{https://github.com/koush/ion}} provides asynchronous networking and image loading for Android.
\item \textbf{jadx}\footnote{\url{https://github.com/skylot/jadx}} is a Java decompiler for Android Dex and Apk files.
\item \textbf{mapstruct}\footnote{\url{https://github.com/mapstruct/mapstruct}} is an annotation processor for generating type-safe bean mapping classes.
\item \textbf{nettosphere}\footnote{\url{https://github.com/Atmosphere/nettosphere}} provides a WebSocket/HTTP server based on Atmosphere and Netty Framework.
\item \textbf{parceler}\footnote{\url{https://github.com/johncarl81/parceler}} is a library for creating serialize code.
\item \textbf{retrolambda}\footnote{\url{https://github.com/orfjackal/retrolambda}} provides a backport for lambda expressions implemented in Java 8 to Java 7, 6 and 5. 
\item \textbf{ShowcaseView}\footnote{\url{https://github.com/amlcurran/ShowcaseView}} is a library for Android that can highlight and showcase components within the UI of a application.
\item \textbf{smile}\footnote{\url{https://github.com/haifengl/smile}} stands for Statistical Machine Intelligence and Learning Engine and is a machine learning library for Java.
\item \textbf{spark}\footnote{\url{https://github.com/perwendel/spark}} a tiny web framework for Java 8.
\item \textbf{storm}\footnote{\url{https://github.com/apache/storm}} from apache real time computational system for continuous streams of data. This project is one of the larger projects and has a large development community.
\item \textbf{tempto}\footnote{\url{https://github.com/prestodb/tempto}} A testing framework for SQL databases running on Hadoop.
\item \textbf{yardstick}\footnote{\url{https://github.com/gridgain/yardstick}} is a framework for creating benchmarks specifically for clustered or distributed systems.

\end{enumerate}

\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Project & Avg \# of & Avg \# of & Avg \# of & Max & Min \\
         & Methods Change & Changes & Commits / & Commits & Commits \\
         & / Year & / Method & Developer & / Year & / Year \\
        \hline
        acra & 600.83 & 4.52 & 13.93 & 119 & 33 \\
        arquillian-core & 832.13 & 2.03 & 36.38 & 175 & 6 \\
        blockly-android & 4839.5 & 4.68 & 98.71 & 690 & 1 \\
        brave & 1955.75 & 4.24 & 14.65 & 108 & 56 \\
        cardslib & 1707.33 & 3.28 & 46.71 & 223 & 3 \\
        dagger & 1578.5 & 5.64 & 16.0 & 236 & 4 \\
        deeplearning4j & 20549.5 & 5.69 & 65.24 & 2018 & 65 \\
        fresco & 4139.0 & 1.49 & 156.5 & 313 & 313 \\
        governator & 2189.2 & 4.11 & 24.84 & 159 & 75 \\
        greenDAO & 1437.5 & 3.94 & 138.33 & 137 & 5 \\
        http-request & 348.0 & 2.56 & 39.0 & 108 & 5 \\
        ion & 1086.75 & 3.31 & 40.0 & 253 & 7 \\
        jadx & 2330.5 & 2.41 & 43.64 & 208 & 11 \\
        mapstruct & 2037.0 & 2.04 & 54.91 & 288 & 7 \\
        nettosphere & 571.4 & 4.37 & 37.33 & 118 & 5 \\
        parceler & 769.0 & 2.43 & 45.6 & 76 & 41 \\
        retrolambda & 647.0 & 3.06 & 25.0 & 133 & 24 \\
        ShowcaseView & 534.4 & 5.9 & 10.71 & 141 & 6 \\
        smile & 1293.0 & 1.86 & 19.75 & 121 & 6 \\
        spark & 1525.67 & 3.92 & 7.25 & 171 & 22 \\
        storm & 10007.4 & 5.93 & 15.47 & 948 & 118 \\
        tempto & 1693.0 & 1.88 & 16.56 & 253 & 45 \\
        yardstick & 608.0 & 3.65 & 23.67 & 208 & 5 \\
        \hline
    \end{tabular}
\end{center}
\caption{Project Change Statistics 2}
\label{tab:project_stats_2}
\end{table}

% TODO reference the table
Several average measures were also taken which detail the amount of change that occurs within the project. The average number of commits per project coupled with the average number of changes per commit clearly indicates the amount of changes that are occurring with in the project. The rate at which methods are change provides good insight into the growth of a project. While some changes may involve the addition of new methods, others may include the removal of methods or the modification of methods. The other measures relating to the amount of change occurring with a project on average are the number of methods changed per year and the number of changes per method. Each of these further outline how the changes are being made to the project on average.

A few of the measures are related to the number of developers. These while provided are not the primary focus. The information provided by tracking developer interactions with each other or the repository could be integrated into future work.

\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Project & Max \# of & Min \# of & Max \# of & Max \# of & Min \# of \\
         & Methods & Methods & Change / & Commits / & Commits / \\
         & Changed & Changed & Method & Developer & Developer \\
         & / Year & / Year & & & \\
        \hline
        acra & 1503 & 183 & 52 & 229 & 1 \\
        arquillian-core & 3421 & 55 & 110 & 420 & 1 \\
        blockly-android & 9543 & 136 & 90 & 538 & 1 \\
        brave & 3300 & 1038 & 83 & 225 & 1 \\
        cardslib & 3340 & 19 & 95 & 285 & 1 \\
        dagger & 3374 & 171 & 65 & 157 & 1 \\
        deeplearning4j & 35869 & 4377 & 345 & 1987 & 1 \\
        fresco & 4139 & 4139 & 33 & 269 & 44 \\
        governator & 3324 & 1066 & 263 & 316 & 1 \\
        greenDAO & 2971 & 34 & 63 & 367 & 1 \\
        http-request & 752 & 14 & 50 & 267 & 1 \\
        ion & 2315 & 24 & 161 & 492 & 1 \\
        jadx & 3915 & 248 & 197 & 436 & 1 \\
        mapstruct & 4462 & 201 & 81 & 334 & 1 \\
        nettosphere & 1074 & 10 & 46 & 322 & 1 \\
        parceler & 1151 & 516 & 31 & 217 & 1 \\
        retrolambda & 1501 & 212 & 83 & 237 & 1 \\
        ShowcaseView & 1156 & 74 & 70 & 215 & 1 \\
        smile & 1918 & 872 & 24 & 155 & 1 \\
        spark & 2818 & 277 & 72 & 277 & 1 \\
        storm & 26526 & 2152 & 314 & 622 & 1 \\
        tempto & 3073 & 313 & 44 & 66 & 1 \\
        yardstick & 1163 & 53 & 62 & 137 & 1 \\
        \hline
    \end{tabular}
\end{center}
\caption{Project Change Statistics 3}
\label{tab:project_stats_3}
\end{table}

While the purposed method was being developed ACRA's acra project was primarily used for exploring and initial testing of the approach. After experimenting on acra a few of the potential candidate feature sets were distinguished based on their superior performance. Experiments were then run on other projects using the feature sets that performed better.

\section{Experimental Setup}


The experiment is setup to have a set of parameters that can be set. These parameters will remain constant to observer the difference that the independent variable will have on the dependent variables; precision, recall and accuracy. Each experiments will use one of the parameters as the independent variable. An experiment consists of a set of trails where the independent variable is modified to measure the resulting dependent variables. The results of a single trial or of the set of trails for a project will be referred to as the performance of the approach. In order to reduce specific project confounding factors numerous projects were tested on. This will be discussed further in the discussions section. The experiments section only includes a small sample of the projects that were experimented on. The projects shown are ones that exhibited interesting patterns or results. The complete set of performance figures for every project are shown in \autoref{app:experimental_data}.

This thesis works to determine whether \gls{svm} or \gls{rf} can be used to effectively predict changes that will occur within the project. To potentially provide an answer to this question the factors that are used for the prediction method are studied. The experiments attempt to determine what impact the different factors will have on the purposed methods. These factors include:
\begin{enumerate}
\item The \gls{swr} which is the size of range which the samples are taken from.
\item The set of features used to train the machine learning model.
\item The distribution of the data through use of \gls{os}.
\end{enumerate}

Through investigating these factors a more clear picture of the performance of the approach will be be provided. Without such a investigation the method contains could produce capable solution just as likely as poor solutions. Worse still, the setup may produce poor solutions more often than capable solutions. Once a more concrete understanding is developed of the different factors and the performance of the algorithm accordingly the research question can be answered as to whether it is possible to predict changes within a project using the commit data.

\subsection{Prediction Features}

%The experimental process for testing the various feature sets on acra involved dividing the repository into four equal quarters (based on time duration). While the number of commits within each period may vary greatly, only a sample is taken.

    %- Alternatively the periods could be distributed such to make the number of commits equal between the sections. Further testing is needed to determine which method is more ideal.

%The \gls{svm} model was trained to categorize whether the current method would be changed within the next 5 commits. This of course can generalize to whether a vector will have a commit within the next \textit{n} commits. Obviously each candidate feature leverages historical or current information and thus a vector can be generated without future information.


%TODO write algorithm for mapping names

% TODO discuss sampling range methods



%An issue that was necessary to address was the arbitrary sample size. For projects that are a lot bigger 100 vectors which map to 100 method changes could be very small. The sampling also seemed like a peculiar approach to picking the data since it would randomly pick values from over a period that could vary from a few months to a few years depending on the size. Therefore instead of dividing the project into four quarters based on time a number of commits is picked. 

% TODO provide an actual start to this subsection
The experimental design to allow for the predictions made on historical data to be tested with available data. Therefore within the data collected for the project the predictions must be made for values that are already known to allow for verification. Therefore the experimental sampling would build off of the prediction sampling outlined in \autoref{sec:prediction_data}. A second region defined as the prediction sample range. The \autoref{fig:data_range} outlines the updated layout. The size of the training range ($|s_t|$) and the size of the prediction ($|s_p|$) are able to be different sizes, however for each experiment they remain the same ($|s_t| = |s_p|$).

%The population is sampled by percentage of the total available. This allows more flexibility and determining the sample size of a test by allowing for it to scale based on the size of the project. 

% TODO show a final picture with population size as a percentage.

% TODO access this stuff below. Remember It's now 5 commits not 6 and also a lot of this stuff is no longer relevant.

%The initial thought was to provide a few different features that appeared to be unique and potentially provided useful information for whether the method will change within the next 5 commits. Of course since this measurement is calculated, if a vector within the sample set is within the last 5 commits then it will leverage data from the next quarter to provide its prediction. This has not been mitigated and could provide a unrealistic improvement in the prediction score if members of the next sample fall into the first 5 commits. The way to mitigate this would be to provide a buffer between the two sets when the second test is used for testing purposes. The second set would be restricted further, such that the changes must come from after the 6th commit from the start date of the quarter. The first commit would be the one that takes place on or right after (if no commit falls on that date) the start date. The next 5 commits would also be excluded from the test sample set.

%\subsection{Experiment Factors}
% TODO outline the different factors used in the experiments. Also even outline other factors that are not changed either.

% Factors:
% Sliding Window
% Extended sampling ranges
% Training Range
% Testing Range
% Sample Range by Commit (or by date)
% Oversampling
% Undersampling
% Dynamic Size of sampling
% Sampling Percent
% Sampling Randomly
% Window
% SVM or Forest Parameters

The sample range is taken from the current commit $c_i$ to $c_{i-g-m}$ in the case that $i > m$. $m$ denotes the size of \gls{swr} in commits and $g$ is the number of commits the change is predicted within. For example if the model is predict a change that occurs within the next 5 commits ($g = 5$) and $m = 30$ then \autoref{fig:data_range} shows how the data would be sampled. The training sample would be where data would be collected from to train the model. The prediction gap is to account for the data sampling calculating whether methods at commit 40 will have a change within the next 5 commits. Therefore to properly test it on data that is not used as part of the testing model the offset is needed. The \gls{swr} for the testing data set is labeled as the \textit{Testing Sampling}.

% TODO include a table/list about each of the features (similar to tab:candidate_features)

The sliding window factor is one of core aspects related to extracting samples from the data set. When using the sliding window to sample the data the data is divided as shown in \autoref{fig:data_range}. The training sample is where the training data set is sampled from. The testing sample is where the testing data is sampled from. 

\begin{figure}[!ht]
    \centering
        %Generated from http://jsfiddle.net/toufm01e/
        \includegraphics[width=1.0\textwidth]{images/exp_data_range}
    \caption{Sampling Window Layout}
    \label{fig:data_range}
\end{figure}


A data set with an extended sampling range will extend the sampling range beyond the original size for either the training sample or the testing sample. The training range can be expanded to include earlier samples to increase the sample space.

The training and testing sampling range are defined as the number of commits from which the samples can be taken. In \autoref{fig:data_range}, both the training and testing sample ranges are set to 30 commits. These two values can differ from one another but tend to be kept the same for most of the experiments.

As discussed in \autoref{sec:prediction_data}, sample biasing can cause the distribution to favor the selection of one category over another. Undersampling and \gls{os} can prevent the model from simply classifying all samples as one category or the other.

For each project data set there are numerous windows that be can be used. The window number is setting which window is used broadly mapping to the position within the data set that the model will be trained on and then predicted on. In \autoref{fig:data_range} the \textit{current commit} is located at 45. This is the point from which predictions will be made after. The gap preceding the starting point is is 5 commits long and is followed by the sample window for the training data which is 30 commits long. To calculate the window offset simply using the starting position ($p_s$), the gap length ($g$), and the \gls{swr} can be calculated in \autoref{eq:window_offset}. Therefore in this case the window offset would be $45 - 5 - 30$ which is 10.

\begin{equation} 
\label{eq:window_offset}
wo = p_s - g - swr 
\end{equation}

Finally, the last factor of note is the parameters used to configure each prediction method. \gls{rf} use a single parameter, the size of the forest. \gls{svm} meanwhile uses two parameters; C and gamma. Picking the most suitable parameters is ideal to achieve good performance from the prediction model. For \gls{svm} a grid search technique is provided by the developers of the libsvm source % TODO cite easy.py
for optimizing the parameters. For \gls{rf}, the size of the forest will have an impact but is far more manageable since it is a single parameter. A larger number of trees in the forest will generally provide better results, but will cause the algorithm to take longer to train.

\subsection{Prediction Performance}

% TODO this isn't necessary for cases when the entire sample is used.
For each experiment where the used random sampling the experiment was performed 5 times to account for variations in the random sample. Therefore if the initial results using the first sample set were not characteristic of the full dataset then running the experiment with more random samples is more likely to represent the true characteristics of the dataset. This required taking five random samples from each quarter, training the model and running the tests on the model to then determine the average prediction score. %In the case when $100\%$ of the sample is used then only one sample is is taken since there will be no variations within the sample set.

The goal of the prediction methods are to provide a good prediction of whether the a given vector will fit in one category or the other. A model's prediction performance can be rated using three measures of accuracy, precision and recall. Accuracy is measured as how often predictions $p_i$ are classified correctly where $a_i$ represents vector $v_i$ correct classification. The prediction accuracy ($P_{accuracy}$) can then be calculated using \autoref{eq:prediction_accuracy}. This simply sums up the accuracy for each vector and then divides it by the total number of vectors (where $n = |v|$).

% TODO integrate these
\begin{equation}
\label{eq:true_positive}
tp = \sum_{i=0}^{n}\left\{\begin{matrix}
1 & \text{if } p_i = a_i \text{ \& } a_i = 1\\ 
0 & \text{otherwise}
\end{matrix}\right.
\end{equation}

\begin{equation} 
\label{eq:true_negative}
tn = \sum_{i=0}^{n}\left\{\begin{matrix}
1 & \text{if } p_i = a_i \text{ \& } a_i = 0\\ 
0 & \text{otherwise}
\end{matrix}\right.
\end{equation}

\begin{equation}
\label{eq:false_positive}
fp = \sum_{i=0}^{n}\left\{\begin{matrix}
1 & \text{if } p_i \neq a_i \text{ \& } a_i = 0\\ 
0 & \text{otherwise}
\end{matrix}\right.
\end{equation}

\begin{equation}
\label{eq:false_negative}
fn = \sum_{i=0}^{n}\left\{\begin{matrix}
1 & \text{if } p_i \neq a_i \text{ \& } a_i = 1\\ 
0 & \text{otherwise}
\end{matrix}\right.
\end{equation}

\begin{equation}
\label{eq:prediction_accuracy}
P_{accuracy} = \frac{tp+tn}{tp+tn+fp+fn} \times 100
\end{equation}

The precision of a model is the measure of how correct the model predicts that a change will occur when it predicts that a change will occur. Given the true positives $tp$, represents the number of predictions that the model correctly identified as having a change and the false positives $fp$ is the number of times the model incorrect predicted a change to occur when it in fact did not. The equation for calculating precision is show in \autoref{eq:precision}.



\begin{equation} 
\label{eq:precision}
P_{precision} = \frac{tp}{tp+fp}
\end{equation}

The recall of the model is the measure of how correct the model predicts that change will occur out of all the times changes really occurred. Again using $tp$ as the number of true positives, and false negatives $fn$ which is the number of times the model fails to predict that a change will occur. The recall can be calculated using the \autoref{eq:recall}.

\begin{equation} 
\label{eq:recall}
P_{recall} = \frac{tp}{tp+fn}
\end{equation}

\section{Experimental Results}
\label{sec:experimental_results}

% TODO give a brief introduction to experiments. Reference the data discussed in previous sections in this chapter.
% TODO reference the appendix which contains a complete set of figures related to the experiments conducted.

For each experiment all of the data used to train and test the model is collected using a Ruby script to query a PostgreSQL database. The PostgreSQL database provides the raw data which is then processed into data vectors in an acceptable form for \gls{svm} or \gls{rf}. The data processing method is outlined more completely in \autoref{sec:prediction_method}.

\subsection{SVM Experiments}
\label{subsec:svm_experiments}

For this set of experiments the machine learning algorithm \gls{svm} is used to provide the change predictions. As noted in \autoref{sec:prediction_method}, the implementation for \gls{svm} is a Ruby binding of the original library. The parameters used for all of the experiments with \gls{svm} are $C = 10$ and $gamma = 8$.


\subsubsection{Window Range Experiments}
\label{sec:svm_swr_experiment}


In this experiment the independent variable is the size of the \gls{swr} in commits. For each variation of the \gls{swr} the performance is measured. In \autoref{tab:svm_window_range_experiment_features}, the features used by the prediction model are outlined. Features with a mark, $\bullet$, are used while those without are not. In this experiment only the $sf_{\Delta}$ is not used while all the rest are. Each of these features is outlined in further detail in \autoref{tab:candidate_features}.

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
         %& & & & & & & \\
        \hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Features}
    \label{tab:svm_window_range_experiment_features}
\end{center}
\end{table}

As noted above the independent variable for this experiment is the \gls{swr}. The remaining parameters for the experiment are constant for each test. These parameters for this experiment are outlined in \autoref{tab:svm_window_range_experiment_setup}.

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|cc|}
        \hline
        Extended & Over & Under & Sample & Window & SVM & \\
        Window & Sampling & Sampling & Rate & Offset & C & gamma \\ \hline
        No & No & Yes & $100\%$ & 5 & 10 & 8 \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Setup}
    \label{tab:svm_window_range_experiment_setup}
\end{center}

\end{table}

Each project was tested on using these outlined parameters for an \gls{swr} varying from 60 to 130 by intervals of 10. The results for the experiments are shown with the precision, recall and accuracy. For each graph the independent variable is the number of commits in the \gls{swr}. Y-axis is the percentage for either the precision, recall or accuracy. The complete set of experimental performance results are found in \autoref{app_sub:experiment_1_svm}. For some projects did not have enough data to complete the entirety of this experiment. For example, smile did not have enough data to complete the trials with \gls{swr} for 120 or 130. These projects were still included and show how the method works with smaller amount of data available.

% List of projects for this section:
% tempto *pre-groups 1
% http-request *group 2
% acra *group 3
% smile *group 4-5
% spark * group 6+

% TODO rework the following to be the key figures.

The majority of the projects using \gls{svm} did not perform well with accuracy and precision typically between $0.4$ and $0.6$. This project as well as others have very poor results and show the difficulty of this problem. Similarly, \autoref{fig:test_1_blockly-android_svm} shows low precision and accuracy while very high recall. The independent variable, \gls{swr} has very little impact on the performance for this project in particular.


\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_1/tempto_sample_range}
        \caption{\gls{swr} for tempto using \gls{svm}}
        \label{fig:test_1_tempto_svm}
\end{figure}

\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_1/blockly-android_sample_range}
        \caption{\gls{swr} for blockly-android using \gls{svm}}
        \label{fig:test_1_blockly-android_svm}
\end{figure}

Some of the projects like http-request in \autoref{fig:test_1_http-request_svm} had a large amount of variation with the changes to the \gls{swr}. In one case http-request moderately well in \gls{swr} 80 while at 60 and 120 the accuracy and recall are $0$.

\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_1/http-request_sample_range}
        \caption{\gls{swr} for http-request using \gls{svm}}
        \label{fig:test_1_http-request_svm}
\end{figure}

In \autoref{fig:test_1_acra_svm}, the project acra is shown with the best result for \gls{svm}. When the \gls{swr} is at 70-100 the performance is high, for the other cases the performance is lower but not by a large margin. The point of interest is that recall performs well for an \gls{swr} of 100 or lower but performs worse than the accuracy and precision after 100.

\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_1/acra_sample_range}
        \caption{\gls{swr} for acra using \gls{svm}}
        \label{fig:test_1_acra_svm}
\end{figure}

\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_1/smile_sample_range}
        \caption{\gls{swr} for smile using \gls{svm}}
        \label{fig:test_1_smile_svm}
\end{figure}

\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_1/spark_sample_range}
        \caption{\gls{swr} for spark using \gls{svm}}
        \label{fig:test_1_spark_svm}
\end{figure}

Both smile in \autoref{fig:test_1_smile_svm} and spark performed poorly with each performance measure below $0.5$. In two cases (90 and 110) smile and 0 recall and undefined precision.

Overall there was no clear value for the \gls{swr} which held consistent positive results. Projects from similar groups tended to perform similarly. For example acra, dagger and ShowcaseView all tended to perform well for similar parameters. 

Projects that were influenced more by \gls{swr} thus having a larger variation between values proved to have better results more often however this was not guaranteed. No value of \gls{swr} works across projects and even for projects that worked the correct value had to be found in order to obtain good results.

    
\subsubsection{Feature Set Experiments}
\label{sec:svm_feature_set_experiments}


\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|cc|}
        \hline
        Extended & Over & Under & Sample & Window & \gls{swr} & SVM & \\
        Window & Sampling & Sampling & Rate & Offset &  & C & gamma \\ \hline
        No & No & Yes & $100\%$ & 5 & 90 & 10 & 8 \\ \hline
    \end{tabular}
    \caption{Feature Experiment Setup}
    \label{tab:svm_feature_experiment_setup}
\end{center}

\end{table}

% Extend Window: No% Sample by Commit Range: Yes
% Over Sampling: No
% Under Sampling: Yes
% Sample Rate: 100%
% Window Offset: 5
% SVM c: 10
% SVM gamma: 8
% SVM esp: 0.001

%The parameter for this experiment are outlined in \autoref{tab:window_range_experiment_setup}. The major difference between the \gls{svm} and this experiment, \gls{rf}, is the parameters used for the \gls{rf}. This allows for a fairly clear comparison between these two methods with the given independent variable, sample window size.

This experiment uses different sets of candidate feature to test to explore the available features. The remaining variables were kept constant to allow for the candidate feature sets to be viewed in isolation. These constants are provided in \autoref{tab:svm_feature_experiment_setup}. The value of $90$ was selected for the \gls{swr} based on the value being in the middle of the range experimented on for the previous experiment. The remaining variables are kept the same as the previous experiment in \autoref{sec:svm_swr_experiment}. 

\begin{table}[ht]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Feature & Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        1 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ \\
        2 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\
        3 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        4 & & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        5 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & & & $\bullet$ \\ \hline
        %6 & $\bullet$ & & & & $\bullet$ & $\bullet$ & $\bullet$ & \\ 
    \end{tabular}
    \caption{Candidate Feature Sets}
    \label{tab:svm_feature_experiment_sets}
\end{center}
\end{table}

% TODO update the section below to only contain the key figures
The candidate feature sets are outlined in \autoref{tab:svm_feature_experiment_sets}. These feature sets were selected from a larger set of features outlined in \autoref{sec:prediction_data}. Each set is assigned an index value to allow for easier reference later on. For the remainder of this section the experiment sets will be referenced using the assigned index. Therefore if feature set 3 is referenced then that refers to the candidate feature set in the third row. Some of the projects results are shown in figures below. The rest of this experiments performance results can be found in \autoref{app_sub:experiment_2_svm}.

% Projects to show:
% ShowcaseView
% nettosphere
% deeplearning4j
% ion
% mapstruct

\begin{figure}[!t]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_3/ShowcaseView_sample_range}
        \caption{Feature for ShowcaseView using \gls{svm}}
        \label{fig:test_3_ShowcaseView_svm}
\end{figure}

Show

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/deeplearning4j_sample_range}
    \caption{Feature for deeplearning4j using \gls{svm}}
    \label{fig:test_3_deeplearning4j_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/ion_sample_range}
        \caption{Feature for ion using \gls{svm}}
        \label{fig:test_3_ion_svm}
\end{figure}

The projects ShowcaseView, deeplearning4j and ion were all greatly impacted by the different feature sets. ShowcaseView in \autoref{fig:test_3_ShowcaseView_svm} performed well for feature set 1 and 5 and terribly for feature set 3. 
Similarly for ion in \autoref{fig:test_3_ion_svm}, feature sets 1 and 5 performed well with the rest of the feature sets performing poorly. Finally for deeplearning4j in \autoref{fig:test_3_deeplearning4j_svm}, the best performance was for feature set 3 where as the remaining trails were not as good. There were a few projects like these ones were one or two of the feature sets would perform well. One that performed well for certain feature sets tended to share similar project classifications like ShowcaseView and ion do.

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/nettosphere_sample_range}
        \caption{Feature for nettosphere using \gls{svm}}
        \label{fig:test_3_nettosphere_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/mapstruct_sample_range}
        \caption{Feature for mapstruct using \gls{svm}}
        \label{fig:test_3_mapstruct_svm}
\end{figure}

A lot of projects did not vary greatly for different feature sets providing similar to results to that of nettosphere in \autoref{fig:test_3_nettosphere_svm}. All three performance measures show little variance and only small chances are present between projects. Other projects performed poorly for all the feature sets such as mapstruct in \autoref{fig:test_3_mapstruct_svm} which had 3 of the 5 trails score lower than $0.5$ in all performance measures. The feature sets are an influencing factor on the performance of the model however no single feature set found to stand out as the ideal candidate for all projects.

\subsubsection{SVM Oversampling Experiment}
\label{sec:svm_os_experiment}

% TODO add other tests to this subsection

\gls{os} is a balancing technique used to increase the amount of samples available. Samples from the smaller data set are re-sampled to increase the size of the data set. While this does introduce duplicates into the model it also counter acts biasing that is present when one classification is more common then the other by a large margin. Under sampling is also used to remove excess elements from the larger set of classification. \gls{os} This is especially useful for data sets that contain a small number of samples for a particular category. In that case under sampling may limit the performance of a model by removing nearly all of the elements in the data set.

\begin{table}[ht]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Project & \multicolumn{1}{c}{\gls{swr}} & \\
         & \multicolumn{1}{c}{Best} & \multicolumn{1}{c|}{Worst} \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        acra & 90 & 130 \\
        dagger & 60 & 70 \\
        fresco & 130 & 90 \\
        storm & 80 & 130 \\
        deeplearning4j & 120 & 130 \\ \hline
    \end{tabular}
    \caption{Best And Worst Results From SWR experiments for SVM}
    \label{tab:svm_best_worst_swr_experiment_sets}
\end{center}
\end{table}

The experiment below took the best and worst trials from \autoref{sec:svm_swr_experiment} and used \gls{os} when sampling the data. For each of these experiments the candidate feature set from \autoref{tab:svm_window_range_experiment_features} was used. In some cases the best performing experiment may not have been entirely clear. For example with some projects having very high recall ($ \geq 0.9$) while having lower precision and accuracy. The best trail was picked on having the all three parameters higher rather than one an of the values being outlier. Of course accuracy and precision are very closely related and therefore all three were attempted to be maximized rather than just two of the three.

\begin{figure}[!t]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_4/acra_sample_range}
        \caption{Oversampling for acra using \gls{svm}}
        \label{fig:test_4_acra_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/dagger_sample_range}
        \caption{Oversampling for dagger using \gls{svm}}
        \label{fig:test_4_dagger_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/fresco_sample_range}
        \caption{Oversampling for fresco using \gls{svm}}
        \label{fig:test_4_fresco_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/storm_sample_range}
        \caption{Oversampling for storm using \gls{svm}}
        \label{fig:test_4_storm_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/deeplearning4j_sample_range}
    \caption{Oversampling for deeplearning4j using \gls{svm}}
    \label{fig:test_4_deeplearning4j_svm}
\end{figure}

The best and worst from \autoref{sec:svm_swr_experiment} to determine the effect of \gls{os}. In the plots for this section the performance of the \gls{svm} model from the experiment in \autoref{sec:svm_swr_experiment} is represented by either \textit{best} or \textit{worst}. Alternatively, the values for the performance of the \gls{svm} model when using \gls{os} is represented by either \textit{best-O} or \textit{worst-O}. In \autoref{tab:svm_best_worst_swr_experiment_sets}, the value of \gls{swr} used for to obtain best and worst. For the experiments the \gls{swr} will be set to the corresponding value from \autoref{tab:svm_best_worst_swr_experiment_sets}. Therefore the project \textit{acra}, the trail for \textit{best} and \textit{best-O} both use an \gls{swr} of $90$. Furthermore, for \textit{worst} and \textit{worst-O} will use an \gls{swr} of $130$.

The majority of the experiments there is little to no impact whether \gls{os} was used or not. The only experiment that \gls{os} had a noticeable impact was deeplearning4j. In \autoref{fig:test_4_deeplearning4j_svm} the performance of both the best and the worst with \gls{os} is worse with the exception of a slight improvement for the recall in the best trials.

\subsubsection{SVM Discussion}
\label{subsec:svm_discussion}

% TODO update this to include results from newly added projects
The three different experiments attempted to determine the impact of the different factors on the prediction method. The three factors that were tested are:
\begin{enumerate}
\item \gls{swr}
\item Model features
\item Sampling balancing
\end{enumerate}
Overall \gls{swr} had the greatest impact on the performance of the prediction method. While the \gls{swr} does have a large impact some projects are less affected. Specifically, the accuracy and precision remained rather stable will primarily small variations for any change in \gls{swr}, model features and sampling balancing. The model's recall however, tended to vary more greatly for the first two experiments. Small changes to the \gls{swr} will result in large changes to the recall. Likewise in some of the trails from the second experiment the recall will change drastically with slightly different sets of features. While positive results were achieved negative results also were present. Furthermore, no clear pattern was discovered to allow for simple configuration of the parameters to provide positive results. Therefore use of this approach with a \gls{svm} model can be beneficial but also incurs a risk associated with poor predictions.   

\subsection{Random Forest Experiments}
\label{sec:rf_experiments}

The machine learning algorithm \gls{rf} is used for the second set of experiments. \gls{rf} was selected as an alternative to \gls{svm} for it's success in various data mining related tools. The implementation of \gls{rf} is in a python library \textit{scikit-learn} which is outlined in \autoref{sec:prediction_method}. Only one parameter is used for \gls{rf}, the forest size, which is set to $10000$ all of these experiments.

\subsubsection{Window Range Experiments}
\label{sec:rf_swr_experiment}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Features}
    \label{tab:rf_window_range_experiment_features}
\end{center}

\end{table}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Extended & Over & Under & Sample & Window & \gls{rf} \\
        Window & Sampling & Sampling & Rate & Offset & Size \\ \hline
        No & No & Yes & $100\%$ & 5 & 10000 \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Setup}
    \label{tab:rf_window_range_experiment_setup}
\end{center}

\end{table}

The independent variable for this set of experiments is the sample window size measured in commits. The feature set are outlined in \autoref{tab:rf_window_range_experiment_features}. The features used for this experiment is the same as the first \gls{svm} experiment feature set.

The parameters for this experiment are outlined in \autoref{tab:rf_window_range_experiment_setup}. The only difference between the parameters used in this experiment and the parameters used in the \gls{svm} experiment one is the \gls{rf} specific parameters. This allows for a fairly clear comparison between these two methods with the given independent variable, the \gls{swr}. The experiment was conducted on all 23 projects collected and examples were are discussed in more detail in this section. The remainder of the projects performance results are outlined in \autoref{app_sub:experiment_1_rf}.

% Projects
% http-request
% jadx
% parceler
% storm
% dagger

\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/rf/test_1/http-request_sample_range}
        \caption{\gls{swr} for http-request using \gls{rf}}
        \label{fig:test_1_http-request_rf}

    \includegraphics[width=0.8\textwidth]{images/rf/test_1/http-request_importance}
        \caption{Feature Importance \gls{swr} for http-request using \gls{rf}}
        \label{fig:test_1_http-request_rf_importance}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/jadx_sample_range}
    \caption{\gls{swr} for jadx using \gls{rf}}
    \label{fig:test_1_jadx_rf}

    \includegraphics[width=0.8\textwidth]{images/rf/test_1/jadx_importance}
        \caption{Feature Importance \gls{swr} for jadx using \gls{rf}}
        \label{fig:test_1_jadx_rf_importance}
\end{figure}

\begin{figure}[!t]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/parceler_sample_range}
        \caption{\gls{swr} for parceler using \gls{rf}}
        \label{fig:test_1_parceler_rf}

    \includegraphics[width=0.8\textwidth]{images/rf/test_1/parceler_importance}
        \caption{Feature Importance \gls{swr} for parceler using \gls{rf}}
        \label{fig:test_1_parceler_rf_importance}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/storm_sample_range}
        \caption{\gls{swr} for storm using \gls{rf}}
        \label{fig:test_1_storm_rf}

    \includegraphics[width=0.8\textwidth]{images/rf/test_1/storm_importance}
        \caption{Feature Importance \gls{swr} for storm using \gls{rf}}
        \label{fig:test_1_storm_rf_importance}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/dagger_sample_range}
        \caption{\gls{swr} for dagger using \gls{rf}}
        \label{fig:test_1_dagger_rf}

    \includegraphics[width=0.8\textwidth]{images/rf/test_1/dagger_importance}
        \caption{Feature Importance \gls{swr} for dagger using \gls{rf}}
        \label{fig:test_1_dagger_rf_importance}
\end{figure}


% TODO discuss the importance figure.

The results for the experiments with the independent variable sample window size using random forest were mixed. Both acra in \autoref{fig:test_1_acra_rf} and dagger in \autoref{fig:test_1_dagger_rf} have strong prediction results. While the remainder of the projects; fresco in \autoref{fig:test_1_fresco_rf}, storm in \autoref{fig:test_1_storm_rf} and deeplearning4j in \autoref{fig:test_1_deeplearning4j_rf} did not perform as well. For these projects the accuracy and precision were lower, the recall was very high.

While acra had positive results several values of \gls{swr}, as the \gls{swr} increased the recall decreased as well. Both acra and dagger also had a larger variation in performance for accuracy and precision than the remaining three projects. The actual difference however was smaller with the largest variation of around $0.2$. The lack of a large variation for different values of \gls{swr} points to the independent variable not having as strong of impact on the performance. The variable is still useful to help provide a better result but is not the main contributer.

\subsubsection{Feature Set Experiments}
\label{sec:feature_set_experiment_rf}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        Extended & Over & Under & Sample & Window & \gls{swr} & \gls{rf} \\
        Window & Sampling & Sampling & Rate & Offset &  & Size \\ \hline
        No & No & Yes & $100\%$ & 5 & 90 & 10000 \\ \hline
    \end{tabular}
    \caption{Candidate Feature Experiment Setup}
    \label{tab:rf_feature_experiment_setup}
\end{center}

\end{table}

%The parameter for this experiment are outlined in \autoref{tab:window_range_experiment_setup}. The major difference between the \gls{svm} and this experiment, \gls{rf}, is the parameters used for the \gls{rf}. This allows for a fairly clear comparison between these two methods with the given independent variable, sample window size.

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Feature & Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        1 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ \\
        2 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\
        3 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        4 & & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        5 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & & & $\bullet$ \\ \hline
        %6 & $\bullet$ & & & & $\bullet$ & $\bullet$ & $\bullet$ & \\ 
    \end{tabular}
    \caption{Candidate Feature Sets}
    \label{tab:rf_feature_experiment_sets}
\end{center}

\end{table}

Similar to the experiment using a \gls{svm} in \autoref{sec:svm_feature_set_experiments}. The experiment parameters are outlined in \autoref{tab:rf_feature_experiment_setup}. The candidate features are likewise outlined in \autoref{tab:rf_feature_experiment_sets}. Each set is assigned an index value to allow for easier reference later on in this section. The candidate feature set will be referenced by the index assigned in the plots and discussions related. The candidate feature sets were used experimented on with each project which are discussed below.

The rest of the results for this experiment are outlined in \autoref{app_sub:experiment_2_rf}.

% TODO update to be the key figures.

\begin{figure}[!t]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/acra_sample_range}
        \caption{Feature for acra using \gls{rf}}
        \label{fig:test_3_acra_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/dagger_sample_range}
        \caption{Feature for dagger using \gls{rf}}
        \label{fig:test_3_dagger_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/fresco_sample_range}
        \caption{Feature for fresco using \gls{rf}}
        \label{fig:test_3_fresco_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/storm_sample_range}
        \caption{Feature for storm using \gls{rf}}
        \label{fig:test_3_storm_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/deeplearning4j_sample_range}
    \caption{Feature for deeplearning4j using \gls{rf}}
    \label{fig:test_3_deeplearning4j_rf}
\end{figure}

Generally the experiment results were not particularly strong. In \autoref{fig:test_3_acra_rf} the performance for candidate feature sets 1, 2, 3, 5 were all high and is the performs the best out of all the other projects. For dagger in \autoref{fig:test_3_dagger_rf} the result for candidate feature set 1 is high where as the rest are far worse. The remainder of the projects the accuracy and precision are less than $0.6$. In \autoref{fig:test_3_fresco_rf} and \autoref{fig:test_3_storm_rf} the recall is far more consistently high. However deeplearning4j has generally low results for accuracy, precision and recall.

\subsubsection{Oversampling Experiment}
\label{sec:oversampling_experiment_rf}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\ \hline
    \end{tabular}
    \caption{\gls{os} Experiment Features}
    \label{tab:rf_oversampling_features}
\end{center}

\end{table}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Extended & Under & Sample & Window & \gls{rf} \\
        Window & Sampling & Rate & Offset & Size \\ \hline
        No & Yes & $100\%$ & 5 & 10000 \\ \hline
    \end{tabular}
    \caption{\gls{os} Experiment Setup}
    \label{tab:rf_os_experiment_setup}
\end{center}

\end{table}


This experiment builds on top of the previous experiment in \autoref{sec:svm_swr_experiment} and shares a very similar setup as the previous experiment. The features used for this experiment are outlined in \autoref{tab:rf_oversampling_features}. The experiment parameters are outlined in \autoref{tab:rf_os_experiment_setup}. The best and worst trails for each project are taken from the previous experiment. The value of the \gls{swr} was recorded in \autoref{tab:rf_best_worst_swr_experiment_sets} for each project for the best and worst performance of the \gls{rf} model. The experiment uses these trails that were preformed without \gls{os} to compare against new trials that use the exact same parameters except for the use of \gls{os}.

\begin{table}[ht]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Project & \multicolumn{1}{c}{\gls{swr}} & \\
         & \multicolumn{1}{c}{Best} & \multicolumn{1}{c|}{Worst} \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        acra & 90 & 130 \\
        dagger & 90 & 80 \\
        fresco & 60 & 70 \\
        storm & 60 & 70 \\
        deeplearning4j & 100 & 80 \\ \hline
    \end{tabular}
    \caption{Best And Worst Results From SWR Experiments for RF}
    \label{tab:rf_best_worst_swr_experiment_sets}
\end{center}
\end{table}

The result of a trail with \gls{os} are represented in the figures by either \textit{best-O} or \textit{worst-O}. The results without \gls{os} from the previous experiment are represented with \textit{best} and \textit{worst}.

\begin{figure}[!t]
    \centering

        \includegraphics[width=0.8\textwidth]{images/rf/test_4/acra_sample_range}
        \caption{Oversampling for acra using \gls{rf}}
        \label{fig:test_4_acra_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/dagger_sample_range}
        \caption{Oversampling for dagger using \gls{rf}}
        \label{fig:test_4_dagger_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/fresco_sample_range}
        \caption{Oversampling for fresco using \gls{rf}}
        \label{fig:test_4_fresco_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/storm_sample_range}
        \caption{Oversampling for storm using \gls{rf}}
        \label{fig:test_4_storm_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/deeplearning4j_sample_range}
    \caption{Oversampling for deeplearning4j using \gls{rf}}
    \label{fig:test_4_deeplearning4j_rf}
\end{figure}

The results for acra showed a slight improvement for both the best and worst case when using \gls{os} in \autoref{fig:test_4_acra_rf}. While the improvements were marginal they are still present nonetheless. Similarly, fresco in \autoref{fig:test_4_fresco_rf} also had a slight improvement in the performance for all three parameters for both \textit{best-O} and \textit{worst-O}.

While both acra and fresco had slight improvements, dagger in \autoref{fig:test_4_dagger_rf} and storm in \autoref{fig:test_4_storm_rf} both had decreases in performance when \gls{os} was employed. The decrease in performance for the precision, accuracy and recall was small. Storm did have a slight improvement in the recall for \textit{worst-O} over \textit{worst}. The final project, deeplearning4j, had mixed results with the use of \gls{os}. In \autoref{fig:test_4_deeplearning4j_rf}, the performance for \textit{best-O} was worse than the original experiment. However the performance of the \textit{worst-O} was marginally better than the original.

\subsubsection{Random Forest Discussion}
\label{subsec:rf_discussion}

The approach was experimented on using the machine learning algorithm \gls{rf}. The three factors; \gls{swr}, feature set and \gls{os} were investigated. The first factor, \gls{swr}, had positive results for the two smaller projects and lower performance for the remaining projects. The changes made to the \gls{swr} caused little variation between samples. Of course there were some exceptions, for example the recall for acra decreased at $80$, $100$ and $120$. Each drop was around $0.2$ and in total led to a fall of $0.8$. This should however not lessen the positive performance achieved especially in projects like acra.

The second experiment also had very small variability for different values of the independent variable. This time however the independent variable was the feature set used to train and predict with the \gls{rf} model. The accuracy and precision for each project's trial set experienced very small changes. The only two projects to experience a higher amount of variability for the accuracy and precision were acra and dagger. For acra the third feature set decreased the performance. For dagger the first, third and fifth all decreased the performance to around the same level with the second and fourth both providing better performance at around the same level as well. The recall remained high for all projects except for dagger and deeplearning4j. In dagger the recall for the fourth feature set is far lower than the other feature sets at $0.45$. Finally, deeplearning4j for the first feature set has a recall of around $0.5$ and for the fifth feature set around $0.55$. The different feature sets lead to small variations in performance for the \gls{rf}. No clear relationship was determined however, since each project will perform different for given feature sets. For example, acra performs the best with feature set 4. Alternatively, dagger performs best with feature set 2 (which acra also performed well with). For the remaining 3 projects very variation occurs between performance for the 5 feature sets. Another factor of interest is that both acra and dagger had performed well for in the initial experiment using an \gls{swr} of 90. The other three projects which did not perform as well in this experiment also did not perform well at the given \gls{swr} value of 90.

Finally for the third experiment the factor of \gls{os} was investigated. The best and the worst results were taken from the first experiment. \gls{os} was applied to using the same parameters as before. While some projects saw small improvements with the use of \gls{os}, others saw the performance reduce or stay the same. Overall this experiment shows that \gls{os} provides little impact that is inconsistent per project. Since the any increase or decrease to the performance were marginal the impact of \gls{os} on the project is small. While use of \gls{os} for some projects provided an improvement, others the use of \gls{os} was a detriment. The The reason for this could have to do with \gls{rf} being a fairly robust to biasing. However Since the original experiment was already balanced using under sampling the experiment had less to do with the balance of the data set and more to do with the number of samples used. As noted earlier, when both \gls{os} and undersampling are used together then the number of samples re-sampled from the smaller category will be at most double the original amount.

\subsection{Experiment Discussions}

The best performance each project out of the experiments conducted are shown in \autoref{tab:project_performance}. Therefore the best parameters are outlined for each project. The best result was calculated through multi-variable optimization. Since the performance measure consisted of three variables the best result would be one that maintained higher results for each. While not the most ideal, a weighted sum was taken of the precision, recall and accuracy outlined in \autoref{eq:ranking_formula}. The weight of precision ($w_{p}$) and accuracy ($w_{a}$) are closely related both were assigned a weight of $0.5$ while the weight for recall ($w_{r}$) was assigned $1.0$. The higher the resultant summation of the performance measures the higher the ranking of the given performance. Therefore the trails that had the highest performance weighted sum were considered to have performed the best.

\begin{equation} 
\label{eq:ranking_formula}
score_i = w_{pi} \times p_i + w_{ri} \times r_i + w_{ai} \times a_i
max(score)
\end{equation}

While the weighted sum did optimize each value, a common issue was that often a project would have one or two parameter perform well while the remaining performed poorly. For example blockly-android had the maximum value for recall but $0.51$ for precision and accuracy. The weighted sum of this vector would be $0.51 \times 0.5 + 1.0 \times 1.0 + 0.51 \times 0.5 = 1.51$. However in the event that another performance scored $0.7$ for each measure, the weighted sum would be $0.7 \times 0.5 + 0.7 \times 1.0 + 0.7 \times 0.5 = 1.4$. Since the goal is for multi-variable optimization a model that is generally good on each measure is better than a model performs well with one or two but terribly for the rest. Regardless though, some generally performed poorly and that is reflected in the table with a best performance that is quite low or very bias. 

\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        Project & AI & Feature Set & SWR & Test Setup & Precision & Recall & Accuracy \\
        \hline
        acra & SVM & 2 & 100 & test 1 & 0.74 & 0.92 & 0.8 \\          % top tier
        arquillian-core & RF & 5 & 90 & test 3 & 0.53 & 0.97 & 0.56 \\
        blockly-android & SVM & 2 & 60 & test 1 & 0.51 & 1.0 & 0.51 \\
        brave & RF & 2 & 90 & test 1 & 0.6 & 0.95 & 0.65 \\
        cardslib & RF & 2 & 100 & test 1 & 0.65 & 0.74 & 0.67 \\
        dagger & SVM & 1 & 90 & test 3 & 0.51 & 0.9 & 0.52 \\
        deeplearning4j & RF & 2 & 120 & test 1 & 0.57 & 0.93 & 0.61 \\
        fresco & RF & 2 & 60 & test 1 & 0.52 & 1.0 & 0.53 \\
        governator & RF & 2 & 60 & test 1 & 0.57 & 0.81 & 0.6 \\
        greenDAO & SVM & 3, 4 & 90 & test 3 & 0.5 & 1.0 & 0.5 \\
        http-request & RF & 2 & 80 & test 1 & 0.87 & 0.79 & 0.84 \\   % top tier
        ion & RF & 2 & 90 & test 3 & 0.72 & 0.73 & 0.72 \\            % top tier
        jadx & SVM & 2 & 130 & test 1 & 0.55 & 0.82 & 0.58 \\
        mapstruct & RF & 1 & 90 & test 3 & 0.54 & 0.95 & 0.57 \\
        nettosphere & RF & 2 & 110 & test 1 & 0.63 & 0.67 & 0.63 \\
        parceler & SVM & 1 & 90 & test 3 & 0.57 & 0.92 & 0.61 \\
        retrolambda & SVM & 2 & 80 & test 1 & 0.52 & 0.96 & 0.53 \\
        ShowcaseView & SVM & 1 & 90 & test 3 & 0.73 & 0.89 & 0.78 \\  % top tier
        smile & SVM & 5 & 90 & test 3 & 0.53 & 0.96 & 0.55 \\
        spark & SVM & 2, 4 & 90 & test 3 & 0.5 & 1.0 & 0.5 \\
        storm & RF & 2 & 60 & test 1 & 0.61 & 0.89 & 0.66 \\
        tempto & RF & 2 & 70 & test 1 & 0.58 & 0.65 & 0.59 \\
        yardstick & SVM & 3 & 90 & test 3 & 0.53 & 0.68 & 0.54 \\
        \hline
    \end{tabular}
\end{center}
\caption{Project Best Performance}
\label{tab:project_performance}
\end{table}

The number of projects that perform the best with \gls{svm} and \gls{rf} are 11 and 12 respectively. As noted above some of the performance results are still quite low for some of the projects because of a lack of success for that project for all of the trials run. All projects did perform better than $50\%$ which is quite a poor result since it is about as good as a coin toss. Out of the best performances, only 6 of the 23 projects have a trial that performs better than $60\%$ for all three measures. Of those 6 only 4 of those perform better than $70\%$ for each performance measure. Of the top 4, http-request performed the best followed by acra, ShowcaseView and ion in descending order. Both acra and ShowcaseView performed best with use of \gls{svm} while http-request and ion perform best with \gls{rf}. For both http-request and ShowcaseView the overall performance is better for \gls{rf} over \gls{svm}. Likewise, ion performed better for \gls{svm} over \gls{rf}. The results were more consistent between \gls{rf} and \gls{svm}. The second feature set used for acra, http-request and ion to gain the best performance while for ShowcaseView the first feature set performed the best. On a final note, each of these 4 projects are categorized with a project length of long 
 
\clearpage
\section{Threats to Validity}
\label{sec:threat_validity}

This wider experimentation also proved to be very beneficial for the analysis of the method since performance was not consistent across all projects. A concerted effort was made to contrasting positive results for one project with negative results. Such a contrast may mitigate the impact of the positive results, however provide the full context and help direct future work in this area.

Each experiment was designed to attempt to provide a robust setup to measure accurately the performance of the approach given the changes to the current factor. The setup was designed to attempt to preventing the influence of other variables beyond the independent variable. The factors that may have had an influence on the experimental results are the third experiment only sampling the extremes (best and worst).

A major concern with the final experiment was that of the sampling of the best and worst results from the previous experiments to test the use of \gls{os}. While the results of the use of \gls{os} should not be discounted, only the sampling the extremes of the previous experiment may have limited the measurable impact of the use of \gls{os}. This experiment could be extended to test the middle performance or even test each trail from the previous experiments.

The differences between the projects prevented a more direct comparison between the projects. Furthermore, as shown through the experiments, certain projects (acra, dagger) generally performed better than other projects (fresco, storm, deeplearning4j). This leads to the conclusion that certain project related factors have a large impact on the performance of the approach. Further investigation into these project specific factors could lead to improved results for the approach.