\chapter{Experiments}
\label{chap:experiments}

Experiments were conducted in order to provide some validation for the proposed approach. The goal of these experiments is to test the whether the historical commit data from an \gls{oss} project can be used towards predicting future changes within the project. In these experiments the approach outlined in the previous chapter, \autoref{chap:prediction}. The experiment was conducted through the use of measuring the factors of the core factors of the approach. Specifically the \gls{swr}, the feature set and categorization balancing using \gls{os}. These three factors are explored for both machine learning algorithms.

\section{Experimental Project Data}
\label{sec:experimental_project_data}

One thing that should be noted for the experiments that were run. Since all of the data was known before the model was even created artificial cut off dates were created to allow for the feature set to be tested as to their effect on the model. A test project, acra (developed by the user ACRA), was chosen to develop the method on. 

%The project data was extracted from GitHub from when the project was initially committed to GitHub (2010-04-18 15:52:18-04) till the cut off day of extraction (2015-06-05 09:02:56-04). The cut off day was the day the data was extracted from GitHub and after of which the data analysis was initiated.

% TODO consider placing this into introduction
\begin{table}[!hbp]
\begin{minipage}{\textwidth}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Owner & Project & Start Date & End Date & \# of & \# of \\
         & & & & Commits & Developers \\
        \hline
        ACRA & acra\footnote{\url{https://github.com/ACRA/acra}} & 2010-04-18 & 2015-06-05 & 404 & 32 \\
        apache & storm\footnote{\url{https://github.com/apache/storm}} & 2011-09-16 & 2015-12-28 & 2445 & 261 \\
        facebook & fresco\footnote{\url{https://github.com/facebook/fresco}} & 2015-03-26 & 2015-10-30 & 313 & 47 \\
        square & dagger\footnote{\url{https://github.com/square/dagger}} & 2012-06-25 & 2016-01-30 & 496 & 39 \\
        deeplearning4j & deeplearning4j\footnote{\url{https://github.com/deeplearning4j/deeplearning4j}} & 2013-11-27 & 2016-02-13 & 3523 & 62 \\
        \hline
    \end{tabular}
\end{center}
\caption{Experiment projects}
\label{tab:project_summary}
\end{minipage}
\end{table}

The complete list of projects that were tested is found in \autoref{tab:project_summary}. The number of commits excludes any commit that lacked a change to a file containing Java code. Since the primary interest was to parse Java code, files containing Java code were used while all other files are ignored. These measures provide a more accurate description of the project in terms of the analysis and predictions made on it. Secondly, the number of developers does not map effectively to what git uses as committers and authors. Instead, the number of developers includes all individuals (removing duplicates) who committed or authored commits to the current project.

%TODO place the tables in better positions.
\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Project & \# of Methods & \# of Methods & Avg \# of & Avg \# of Methods \\
         & & Changes & Commits / Year & Change / Commit \\
        \hline
        acra & 1309 & 3605 & 67.33 & 9.51 \\
        storm & 14599 & 50037 & 489 & 24.03 \\
        fresco & 3463 & 4139 & 313 & 14.73 \\
        dagger & 1827 & 6314 & 99.2 & 13.70 \\
        deeplearning4j & 29896 & 82198 & 880.75 & 24.33 \\
        \hline
    \end{tabular}
\end{center}
\caption{Project Change Statistics}
\label{tab:project_stats}
\end{table}

Each of the projects selected on GitHub using the list of Java projects with a large amount of contributions. Open source projects were targeted to simplify any usage concerns. Therefore in order to be selected the program had to clearly use an \gls{oss} license. Secondly, the program also needed to have at least a 6 months worth of development and at least 300 commits to provide a large enough dataset to analyze. An effort was also made to pick projects of different sizes to provide better tests of various conditions.


In order to get a more detailed understand of the selected projects numerous measures were taken. These measures also allow for each projects to be compared to each other in terms of the development of each of the projects. The size of the project is represented through number of commits, methods. The size of the development team is also provided. The length of each project is shown and most of the measures average on a yearly term.

\begin{enumerate}
\item \textbf{acra} is a Android bug logging tool used with Android applications to capture information related to bugs or crashes. The information is sent to the developers to help them address the issues that their clients encounter while using there application.
\item \textbf{storm} from apache real time computational system for continuous streams of data. This project is one of the larger projects and has a large development community.
\item \textbf{fresco} from facebook is the smallest project with the shortest development period. This project provides a library for using images on Android to attempt to solve limited memory issues with mobile devices.
\item \textbf{dagger} from square is a Java application used to satisfy dependencies for classes to replace the factory model of development.
\item \textbf{deeplearning4j} is a distributed neural network library that integrates Hadoop and Spark. This application is the largest of the 5 projects and provides a large wealth of data to analyze.
\end{enumerate}

\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Project & Avg \# of & Avg \# of & Avg \# of & Max & Min \\
         & Methods Change & Changes & Commits / & Commits & Commits \\
         & / Year & / Method & Developer & / Year & / Year \\
        \hline
        acra & 600.83 & 4.52 & 13.93 & 119 & 33 \\
        storm & 10007.4 & 5.93 & 15.47 & 948 & 118 \\
        fresco & 4139 & 1.49 & 156.5 & 313 & 313 \\
        dagger & 1578.5 & 5.64 & 16 & 236 & 4 \\
        deeplearning4j & 20549.5 & 5.69 & 65.24 & 2018 & 65 \\
        \hline
    \end{tabular}
\end{center}
\caption{Project Change Statistics 2}
\label{tab:project_stats_2}
\end{table}

% TODO reference the table
Several average measures were also taken which detail the amount of change that occurs within the project. The average number of commits per project coupled with the average number of changes per commit clearly indicates the amount of changes that are occurring with in the project. The rate at which methods are change provides good insight into the growth of a project. While some changes may involve the addition of new methods, others may include the removal of methods or the modification of methods. The other measures relating to the amount of change occurring with a project on average are the number of methods changed per year and the number of changes per method. Each of these further outline how the changes are being made to the project on average.

A few of the measures are related to the number of developers. These while provided are not the primary focus. The information provided by tracking developer interactions with each other or the repository could be integrated into future work.

\begin{table}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        Project & Max \# of & Min \# of & Max \# of & Min \# of & Max \# of & Min \# of \\
         & Methods & Methods & Change / & Change / & Commits / & Commits / \\
         & Changed & Changed & Method & Method & Developer & Developer \\
         & / Year & / Year & & & & \\
        \hline
        acra & 1503 & 183 & 52 & 1 & 229 & 1 \\
        storm & 26526 & 2152 & 314 & 1 & 622 & 1\\
        fresco & 4139 & 4139 & 33 & 1 & 269 & 44 \\
        dagger & 3374 & 171 & 65 & 1 & 157 & 1 \\
        deeplearning4j & 35869 & 4377 & 345 & 1 & 1987 & 1 \\
        \hline
    \end{tabular}
\end{center}
\caption{Project Change Statistics 3}
\label{tab:project_stats_3}
\end{table}

While the purposed method was being developed ACRA's acra project was primarily used for exploring and initial testing of the approach. After experimenting on acra a few of the potential candidate feature sets were distinguished based on their superior performance. Experiments were then run on other projects using the feature sets that performed better.

    % An effort was made to not count individuals twice however some users may have changed their user name and thus showed up several times.
% NOTE the number of commits is the number of commits that contained Java code and thus was useful towards parsing.
% Note the number of developers is the total number of developers who both committed or authored commits, aka people who made pull requests are included.

% TODO merge the different experiment notes sections into one subsection

% \subsection{Experiment 1}

% % TODO look for test results for this one
% The first set of experiments were preliminary and used \gls{svm} as the approach for prediction algorithm. They attempted to predict whether a given method would change within the next 6 commits. The test was done one using ACRA's acra (hence forth just called \textit{acra}). The data set was divided into four sections based on the date length. So each section of the data was the project's lifespan divided by 4 long. The sampling method from the data set $data_i$ was to use the first $n$ tuples ordered by date. The value of $n$ was tested at 100 and 1000. The data set $data_i$ contained tuples which had changes and ones that had no changes.

% $|data_i| = \frac{|date_f - date_s|}{4}$

% The results of these experiments were not particularly promising scoring accuracy, precision, recall and recall around $50\%$ or below. The experiment was run such that $data_i$ would be used for training and $data_{i+1}$ would be used for testing. For example $data_1$ would be used to train $model_{1,2}$ and $data_2$ would be used to test the $model_{1,2}$. A slight variation was also tested where $data_i$ trained $model_i$ and was tested use each data set $data_j$ that had $j > i$. This however still provided poor results similar to the original experiment. \autoref{fig:exp_1_data_range} shows how the data is distributed.

% % TODO consider updating this to use the dates in the x axis.
% \begin{figure}[!ht]
%     \centering
%         \includegraphics[width=1.0\textwidth]{images/exp_1_data}
%     \caption{Experiment 1 Data Sample Range}
%     \label{fig:exp_1_data_range}
% \end{figure}

%\subsection{Experiment 2}

% TODO show results
%To address the fixed data sampling used previously random sampling was employed. The number of samples $n$ was not changed from the first experiment. The prediction results improved slightly. The \gls{svm} model also reported an error relating to \textit{max number of iterations reached}. This error indicates that the \gls{svm} model is having trouble effectively separating the two categorizations of data into two distinct sets. Such warnings could mean that the features used for training the model are not linearly separable.

% TODO show a picture of how the data was divided.

% \subsection{Experiment 3}

% % TODO talk about using easy.py
% Further investigation into \gls{svm} lead to a research tool that provided grid search for optimizing the parameters used for a \gls{svm}. The results of these experiments offered improvements over the previous, however required a long time to find the right parameters and works the best with the dataset $data_i$ that was used to find the parameters.

% \subsection{Experiment 4}

% % TODO get experiment results
% Modified the candidate features set to test the \gls{svm} with. Added ones like change frequency, average time between commits, number of commits since last change, time since last change and previous change type. Dropped commit author in favor of using just committer. Finally changed the prediction to predict the whether a change will occur within the next 5 commits. In terms of the methodology, 2 variations were tested. The first one was predicting changes to a methods within the same package. The second one was prediction changes to a method within the same file. Neither of these changes provided substantial improvements since they reduced the available sample size, $|data_i|$, down so that the no reasonable predictions could be made from the reduced set.

% \subsection{Experiment 5}

% The next set of experiments required more complex features which necessitated more complex queries from the database. In fact the database interface in use, MySQL, was unable to implement some of the queries. MySQL only allows 2 levels of nested queries and has a more restrictive data type set. An alternative database interface PostgreSQL was used as a replacement for MySQL. PostgreSQL offers fair more sophisticated data types as well as \gls{cte}. The migration from MySQL to PostgreSQL was simplified through the use of pgloader as mentioned in \autoref{sec:storage}. 

% Another change was the even out the number of samples collected from each category. The data set $data_i$ tended to provide unbalanced categorization of the data. The same number of samples from each $n/2$ was collected to prevent biasing in the data set. A \gls{svm} model like most other machine learning algorithms is susceptible to category biasing. This will occur when the training set consists of $80\%$ of one category and $20\%$ of another. The model will train such that it always predicts the first category. This works out well if the data is always unbalanced in the same way however it will fail to predict the smaller category entirely. Therefore providing an even sample of each category ($50\%$ each) will prevent poor prediction results.

% Another result of determining that the data set was unbalanced in terms of the categorization was to calculate both precision and recall of the results. Accuracy alone only provides a very simple measure of how well the model predicted the samples from $data_{i+1}$. A clearer understanding is available with these three measures. Also with each provided an attempt can be made to optimize all three.

% Finally the last few tests in this experiment set included a new feature. This feature was the difference in time between the previous commit with a change and the latest commit. This feature was not particularly useful however and caused the data to become inseparable.

% At the end of this set of tests it was apparent that a deeper understanding of the candidate features was necessary to improve the results. Therefore an analysis of each candidate feature was performed both on the quality of the feature and the possible relationship with others. \gls{svm} models are particularly sensitive towards dependencies between the variables. It was also necessary to properly convert data into a format that could be used by the database. This was talked about in more detail in \autoref{subsec:svm_prediction}.

% \begin{table}
% \begin{center}
%     \begin{tabular}{|l|l|l|}
%         \hline
%         Project & Sample Size (n) & Accuracy \\
%         \hline  
%         acra & 100 & $70\%$ \\
%         acra & 1000 & $52\%$ \\
%         \hline
%     \end{tabular}
% \end{center}
%     \caption{Experiment 5 Results}
%     \label{tab:experiment_5_results}
% \end{table}

% \subsection{Experiment 6}

% After analyzing the candidate features a more ideal set of features was created. The tests were preformed again using sample sizes of $100$ and $1000$. After the changes were made to the data, the performance improved however some of the features did not prove as useful. However the improvement was marginal and therefore necessitated shift in focus from the features to the prediction method. Specificity the data sampling method was inspected to attempt improve the prediction results. Instead of breaking the data set into four even sets based on the date range the data was divided into two even sets based on date as shown in \autoref{fig:exp_6_data_range}

% \begin{figure}[!ht]
%     \centering
%         \includegraphics[width=1.0\textwidth]{images/exp_6_data_range}
%     \caption{Experiment 6 Data Sample Range}
%     \label{fig:exp_6_data_range}
% \end{figure}

%The results of the experiment are outlined in \autoref{tab:experiment_6_results}. The larger sample with $n = 1000$ is far worse than with $n = 100$. This however was a worse result than when the data set was divided into 4 equal parts.

% \begin{table}
% \begin{center}
%     \begin{tabular}{|l|l|l|}
%         \hline
%         Project & Sample Size (n) & Accuracy \\
%         \hline
%         acra & 100 & $60\%$ \\
%         acra & 1000 & $47\%$ \\
%         \hline
%     \end{tabular}
% \end{center}
%     \caption{Experiment 6 Results}
%     \label{tab:experiment_6_results}
% \end{table}

% \subsection{Experiment 7}

% Reorganized the data sampling method to sample based on commit ranges rather than date ranges. Instead of splitting the data set into four even sections, the sample range is taken from the current commit $c_i$ to $c_{i-m}$ in the case that $i > m$. $m$ denotes the width in commits of the sample space. For example if the model is predict a change that occurs within the next 5 commits and $m = 30$ then \autoref{fig:data_range} shows how the data would be sampled. The training sample would be where data would be collected from to train the model. The prediction gap is to account for the data sampling calculating whether methods at commit 40 will have a change within the next 5 commits. Therefore to properly test it on data that is not used as part of the testing model the offset is needed. The testing sampling section is the same size as the training sampling data set and follows the 5 commit gap.

% \begin{figure}[!ht]
%     \centering
%         \includegraphics[width=1.0\textwidth]{images/exp_7_data_range}
%     \caption{Experiment 7 Data Sample Range}
%     \label{fig:data_range}
% \end{figure}

% Another change to the sampling method was to sample a percentage of the data within the sample rather than a fixed amount. The sample ranges provided a larger range of data to sample and thus sticking to a arbitrary amount of tuples was updated. This however introduced biasing issues with the data set since typically one category or the other had the majority of tuples within the sample. Fixing this issue is talked about in further detail in \autoref{subsec:random_forest_predictions}. Similar to the previous sampling techniques the data set was sampled randomly. Therefore the percentage of data sampled had a large impact on how long it took to train the model but typically also on the prediction results of the model. 

% TODO discuss the results

% \subsection{Experiment 8}

% After more or less establishing a data sampling model the candidate feature set was looked at again in an attempt to improve the prediction results. A few of the candidate features were removed and a minimum candidate feature set was determined which provided the best results for the current test project \textit{acra}.

% % TODO show some results

% % TODO outline the candidate feature set that performed the best for SVM

% The optimal value of $m$ is a more challenging issue since for project which have a large amount of rapid change occurring in the project larger value seems to provide a more positive result. Where as smaller projects or ones that have a slower rate of change tend to do far better with a smaller value of $m$. 

% \subsection{Experiment 9}

% After the results of the previous experiments proved to less consistent for other data sets such as \textit{storm} and \textit{fresco} another prediction algorithm was tested. \gls{rf} as discussed in greater detail in \autoref{subsec:random_forest_predictions} is more capable with unbalanced datasets and is generally more widely used for performing predictions on mined data. % TODO cite this.

% \gls{rf} while proving to be at times easier to get better results also had some of the challenges that the \gls{svm} model experienced. For example the best features to use in the prediction model and also the best commit width ($m$) to optimize the results.

\section{Experimental Setup}


The experiment is setup to have a set of parameters that can be set. These parameters will remain constant to observer the difference that the independent variable will have on the dependent variables; precision, recall and accuracy. Each experiments will use one of the parameters as the independent variable. An experiment consists of a set of trails where the independent variable is modified to measure the resulting dependent variables. The results of a single trial or of the set of trails for a project will be referred to as the performance of the approach. In order to reduce specific project confounding factors numerous projects were tested on. This will be discussed further in the discussions section.

This thesis works to determine whether \gls{svm} or \gls{rf} can be used to effectively predict changes that will occur within the project. To potentially provide an answer to this question the factors that are used for the prediction method are studied. The experiments attempt to determine what impact the different factors will have on the purposed methods. These factors include:
\begin{enumerate}
\item The \gls{swr} which is the size of range which the samples are taken from.
\item The set of features used to train the machine learning model.
\item The distribution of the data through use of \gls{os}.
\end{enumerate}

Through investigating these factors a more clear picture of the performance of the approach will be be provided. Without such a investigation the method contains could produce capable solution just as likely as poor solutions. Worse still, the setup may produce poor solutions more often than capable solutions. Once a more concrete understanding is developed of the different factors and the performance of the algorithm accordingly the research question can be answered as to whether it is possible to predict changes within a project using the commit data.

\subsection{Prediction Features}

%The experimental process for testing the various feature sets on acra involved dividing the repository into four equal quarters (based on time duration). While the number of commits within each period may vary greatly, only a sample is taken.

    %- Alternatively the periods could be distributed such to make the number of commits equal between the sections. Further testing is needed to determine which method is more ideal.

%The \gls{svm} model was trained to categorize whether the current method would be changed within the next 5 commits. This of course can generalize to whether a vector will have a commit within the next \textit{n} commits. Obviously each candidate feature leverages historical or current information and thus a vector can be generated without future information.


%TODO write algorithm for mapping names

% TODO discuss sampling range methods



%An issue that was necessary to address was the arbitrary sample size. For projects that are a lot bigger 100 vectors which map to 100 method changes could be very small. The sampling also seemed like a peculiar approach to picking the data since it would randomly pick values from over a period that could vary from a few months to a few years depending on the size. Therefore instead of dividing the project into four quarters based on time a number of commits is picked. 

% TODO provide an actual start to this subsection
The experimental design to allow for the predictions made on historical data to be tested with available data. Therefore within the data collected for the project the predictions must be made for values that are already known to allow for verification. Therefore the experimental sampling would build off of the prediction sampling outlined in \autoref{prediction_data}. A second region defined as the prediction sample range. The \autoref{fig:data_range} outlines the updated layout. The size of the training range ($|s_t|$) and the size of the prediction ($|s_p|$) are able to be different sizes, however for each experiment they remain the same ($|s_t| = |s_p|$).

%The population is sampled by percentage of the total available. This allows more flexibility and determining the sample size of a test by allowing for it to scale based on the size of the project. 

% TODO show a final picture with population size as a percentage.

% TODO access this stuff below. Remember It's now 5 commits not 6 and also a lot of this stuff is no longer relevant.

%The initial thought was to provide a few different features that appeared to be unique and potentially provided useful information for whether the method will change within the next 5 commits. Of course since this measurement is calculated, if a vector within the sample set is within the last 5 commits then it will leverage data from the next quarter to provide its prediction. This has not been mitigated and could provide a unrealistic improvement in the prediction score if members of the next sample fall into the first 5 commits. The way to mitigate this would be to provide a buffer between the two sets when the second test is used for testing purposes. The second set would be restricted further, such that the changes must come from after the 6th commit from the start date of the quarter. The first commit would be the one that takes place on or right after (if no commit falls on that date) the start date. The next 5 commits would also be excluded from the test sample set.

%\subsection{Experiment Factors}
% TODO outline the different factors used in the experiments. Also even outline other factors that are not changed either.

% Factors:
% Sliding Window
% Extended sampling ranges
% Training Range
% Testing Range
% Sample Range by Commit (or by date)
% Oversampling
% Undersampling
% Dynamic Size of sampling
% Sampling Percent
% Sampling Randomly
% Window
% SVM or Forest Parameters

The sample range is taken from the current commit $c_i$ to $c_{i-g-m}$ in the case that $i > m$. $m$ denotes the size of \gls{swr} in commits and $g$ is the number of commits the change is predicted within. For example if the model is predict a change that occurs within the next 5 commits ($g = 5$) and $m = 30$ then \autoref{fig:data_range} shows how the data would be sampled. The training sample would be where data would be collected from to train the model. The prediction gap is to account for the data sampling calculating whether methods at commit 40 will have a change within the next 5 commits. Therefore to properly test it on data that is not used as part of the testing model the offset is needed. The \gls{swr} for the testing data set is labeled as the \textit{Testing Sampling}.

% TODO include a table/list about each of the features (similar to tab:candidate_features)

The sliding window factor is one of core aspects related to extracting samples from the data set. When using the sliding window to sample the data the data is divided as shown in \autoref{fig:data_range}. The training sample is where the training data set is sampled from. The testing sample is where the testing data is sampled from. 

\begin{figure}[!ht]
    \centering
        %Generated from http://jsfiddle.net/dxxgjewj/2/
        \includegraphics[width=1.0\textwidth]{images/exp_data_range}
    \caption{Sampling Window Layout}
    \label{fig:data_range}
\end{figure}


A data set with an extended sampling range will extend the sampling range beyond the original size for either the training sample or the testing sample. The training range can be expanded to include earlier samples to increase the sample space.

The training and testing sampling range are defined as the number of commits from which the samples can be taken. In \autoref{fig:data_range}, both the training and testing sample ranges are set to 30 commits. These two values can differ from one another but tend to be kept the same for most of the experiments.

As discussed in \autoref{sec:prediction_data}, sample biasing can cause the distribution to favor the selection of one category over another. Undersampling and \gls{os} can prevent the model from simply classifying all samples as one category or the other.

For each project data set there are numerous windows that be can be used. The window number is setting which window is used broadly mapping to the position within the data set that the model will be trained on and then predicted on. In \autoref{fig:data_range} the \textit{current commit} is located at 45. This is the point from which predictions will be made after. The gap preceding the starting point is is 5 commits long and is followed by the sample window for the training data which is 30 commits long. To calculate the window offset simply using the starting position ($p_s$), the gap length ($g$), and the \gls{swr} can be calculated in \autoref{eq:window_offset}. Therefore in this case the window offset would be $45 - 5 - 30$ which is 10.

\begin{equation} 
\label{eq:window_offset}
wo = p_s - g - swr 
\end{equation}

Finally, the last factor of note is the parameters used to configure each prediction method. \gls{rf} use a single parameter, the size of the forest. \gls{svm} meanwhile uses two parameters; C and gamma. Picking the most suitable parameters is ideal to achieve good performance from the prediction model. For \gls{svm} a grid search technique is provided by the developers of the libsvm source % TODO cite easy.py
for optimizing the parameters. For \gls{rf}, the size of the forest will have an impact but is far more manageable since it is a single parameter. A larger number of trees in the forest will generally provide better results, but will cause the algorithm to take longer to train.

\subsection{Prediction Performance}

% TODO this isn't necessary for cases when the entire sample is used.
For each experiment where the used random sampling the experiment was performed 5 times to account for variations in the random sample. Therefore if the initial results using the first sample set were not characteristic of the full dataset then running the experiment with more random samples is more likely to represent the true characteristics of the dataset. This required taking five random samples from each quarter, training the model and running the tests on the model to then determine the average prediction score. %In the case when $100\%$ of the sample is used then only one sample is is taken since there will be no variations within the sample set.

The goal of the prediction methods are to provide a good prediction of whether the a given vector will fit in one category or the other. A model's prediction performance can be rated using three measures of accuracy, precision and recall. Accuracy is measured as how often predictions $p$ are classified correctly where $a_i$ represents vector $v_i$ correct classification. The algorithm for calculating a single vectors accuracy is showing in \autoref{eq:vector_accuracy}. The prediction accuracy ($P_{accuracy}$) can then be calculated using \autoref{eq:prediction_accuracy}. This simply sums up the accuracy for each vector and then divides it by the total number of vectors (where $n = |v|$).

\begin{equation} 
\label{eq:vector_accuracy}
v_i = \left\{\begin{matrix}
1 & \text{if } p_i = a_i\\ 
0 & \text{otherwise}
\end{matrix}\right.
\end{equation}

\begin{equation}
\label{eq:prediction_accuracy}
P_{accuracy} = \frac{\sum_{i=0}^{n}v_i}{n} \times 100
\end{equation}

The precision of a model is the measure of how correct the model predicts that a change will occur when it predicts that a change will occur. Given the true positives $tp$, represents the number of predictions that the model correctly identified as having a change and the false positives $fp$ is the number of times the model incorrect predicted a change to occur when it in fact did not. The equation for calculating precision is show in \autoref{eq:precision}.

\begin{equation} 
\label{eq:precision}
P_{precision} = \frac{tp}{tp+fp}
\end{equation}

The recall of the model is the measure of how correct the model predicts that change will occur out of all the times changes really occurred. Again using $tp$ as the number of true positives, and false negatives $fn$ which is the number of times the model fails to predict that a change will occur. The recall can be calculated using the \autoref{eq:recall}.

\begin{equation} 
\label{eq:recall}
P_{recall} = \frac{tp}{tp+fn}
\end{equation}

\section{SVM Experiments}
\label{sec:svm_experiments}

For this set of experiments the machine learning algorithm \gls{svm} is used to provide the change predictions. As noted in \autoref{sec:prediction_method}, the implementation for \gls{svm} is a Ruby binding of the original library. The data used to train and test the approach is collected using a Ruby script to query a PostgreSQL database. The parameters used for all of the experiments with \gls{svm} are $C = 10$ and $gamma = 8$.


\subsection{Window Range Experiments}
\label{sec:svm_swr_experiment}

% TODO add dependent and independent variables discussion

In this experiment the independent variable is the size of the \gls{swr} in commits. For each variation of the \gls{swr} the performance is measured. In \autoref{tab:svm_window_range_experiment_features}, the features used by the prediction model are outlined. Features with a mark, $\bullet$, are used while those without are not. In this experiment only the $sf_{\Delta}$ is not used while all the rest are. Each of these features is outlined in further detail in \autoref{tab:candidate_features}.

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
         %& & & & & & & \\
        \hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Features}
    \label{tab:svm_window_range_experiment_features}
\end{center}
\end{table}

%% TODO create a table to show this information.
% Extend Window: No% Sample by Commit Range: Yes
% Over Sampling: No
% Under Sampling: Yes
% Sample Rate: 100%
% Window Offset: 5
% SVM c: 10
% SVM gamma: 8
% SVM esp: 0.001

As noted above the independent variable for this experiment is the \gls{swr}. The remaining parameters for the experiment are constant for each test. The parameters for this experiment are outlined in \autoref{tab:svm_window_range_experiment_setup}.

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|cc|}
        \hline
        Extended & Over & Under & Sample & Window & SVM & \\
        Window & Sampling & Sampling & Rate & Offset & C & gamma \\ \hline
        No & No & Yes & $100\%$ & 5 & 10 & 8 \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Setup}
    \label{tab:svm_window_range_experiment_setup}
\end{center}

\end{table}

The results for the experiments are shown with the precision, recall and accuracy. For each graph the independent variable is the number of commits in the \gls{swr}. Y-axis is the value of each of the different plot, either precision, recall and accuracy. The first project, acra, in \autoref{fig:test_1_acra_svm} has the strongest performance out of all the projects. The best performance for acra is at \gls{swr} at 80 commits width.

% \begin{figure}[h]
%     \centeringb

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_1/acra_sample_range}
%         \caption{\gls{swr} for acra using \gls{svm}}
%         \label{fig:test_1_acra_svm}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_1/dagger_sample_range}
%         \caption{\gls{swr} for dagger using \gls{svm}}
%         \label{fig:test_1_dagger_svm}
%     \end{minipage}sliding window factor
% \end{figure}

% TODO improve the image placement.
\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_1/acra_sample_range}
        \caption{\gls{swr} for acra using \gls{svm}}
        \label{fig:test_1_acra_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_1/dagger_sample_range}
        \caption{\gls{swr} for dagger using \gls{svm}}
        \label{fig:test_1_dagger_svm}
\end{figure}

% \begin{figure}[h]
%     \centering

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_1/fresco_sample_range}
%         \caption{\gls{swr} for fresco using \gls{svm}}
%         \label{fig:test_1_fresco_svm}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_1/storm_sample_range}
%         \caption{\gls{swr} for storm using \gls{svm}}
%         \label{fig:test_1_storm_svm}
%     \end{minipage}
% \end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_1/fresco_sample_range}
        \caption{\gls{swr} for fresco using \gls{svm}}
        \label{fig:test_1_fresco_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_1/storm_sample_range}
        \caption{\gls{swr} for storm using \gls{svm}}
        \label{fig:test_1_storm_svm}
\end{figure}

\begin{figure}[h]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_1/deeplearning4j_sample_range}
    \caption{\gls{swr} for deeplearning4j using \gls{svm}}
    \label{fig:test_1_deeplearning4j_svm}
\end{figure}

Alternatively, dagger has abysmal performance with the exception of a \gls{swr} of 60. Fresco prediction performance does not particularly differ greatly. Also, it should be noted that the experiments with a \gls{swr} of 120 and 130 since both resulted in a undefined precision and recall of 0 and accuracy of $0.5$.

Apache had slight variations, however the precision and accuracy never exceeded $0.55$. Finally, deeplearning4j does not have very good performance with low recall results. As shown in the plots, the \gls{swr} has a significant impact on the performance of the model. While the actual width varies widely per project, there are projects with a width that fares worse and others that fare far better.

Overall there was no clear value for the \gls{swr} which held consistent positive results. Generally speaking, projects that were smaller; acra, dagger, fresco, appeared to get positive results with smaller windows. Alternatively, larger projects; storm and deeplearning4j both did fairly poorly with the test range from 60 to 130. The largest project, deeplearning4j also has a slight positive trend towards the end of the range. However, storm does have a negative trend towards the end of the range.

The precision and accuracy remain fairly stable throughout this experiment. While the value of these attributes changes with different values of the \gls{swr} these changes tend to be fairly close and follow a general trend. However for dagger in \autoref{fig:test_1_dagger_svm} the performance of accuracy and precision drops from $0.6 - 0.7$ range to $0.4 - 0.45$ range. The accuracy and precision also do however remain rather stable for the remainder of the experiments in this set.

Finally, the recall for the prediction is very unstable. While both accuracy and precision are very closely related and so will be likely close in value. Recall however will vary from a very low value to a very high value. For example in \autoref{fig:test_1_fresco_svm} the value of recall at \gls{swr} of $60$ is $0.1$ and then jumps to $0.9$ at \gls{swr} of $70$. \autoref{fig:test_1_storm_svm} and \autoref{fig:test_1_deeplearning4j_svm} both also have large changes in the recall but none as drastic as shown in fresco. 

% \begin{figure}[!ht]
%     \centering
%         \includegraphics[width=1.0\textwidth]{images/test_1/dagger_sample_range}
%     \caption{\gls{swr} for dagger}
%     \label{fig:test_1_dagger}
% \end{figure}

% \begin{figure}[!ht]
%     \centering
%         \includegraphics[width=1.0\textwidth]{images/test_1/fresco_sample_range}
%         \caption{\gls{swr} for fresco}
%         \label{fig:test_1_fresco}
% \end{figure}

% \begin{figure}[!ht]
%     \centering
%         \includegraphics[width=1.0\textwidth]{images/test_1/storm_sample_range}
%         \caption{\gls{swr} for storm}
%         \label{fig:test_1_storm}
% \end{figure}

% \subsection{\gls{swr} Experiments 2}

% For this experiment the variable was the size of the \gls{swr} in commits. In \autoref{tab:window_range_experiment_features}, the features used by the prediction model are outlined. Features with a mark, $\bullet$, are used while those without are not. In this experiment only the Short $\Delta_{freq}$ is not used while all the rest are.

% \begin{table}[h]
% \begin{center}

%     \begin{tabular}{|c|c|c|c|c|c|c|c|}
%         \hline
%         Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
%         % & & & & $\Delta_{freq}$ & & & Next \\
%            \hline
%         $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ \\ \hline
%     \end{tabular}
%     \caption{Window Range Experiment Features}
%     \label{tab:window_range_experiment_features}
% \end{center}

% \end{table}

% %% TODO create a table to show this information..

% % Extend Window: No% Sample by Commit Range: Yes
% % Over Sampling: No
% % Under Sampling: Yes
% % Sample Rate: 100%
% % Window Offset: 5
% % SVM c: 10
% % SVM gamma: 8
% % SVM esp: 0.001

% As noted above the independent variable for this experiment is the \gls{swr}. The remaining parameters for the experiment are constant for each test. The parameters for this experiment are outlined in \autoref{tab:window_range_experiment_setup}.

% \begin{table}[h]
% \begin{center}

%     \begin{tabular}{|c|c|c|c|c|cc|}
%         \hline
%         Extended & Over & Under & Sample & Window & SVM & \\
%         Window & Sampling & Sampling & Rate & Offset & C & gamma \\ \hline
%         No & No & Yes & $100\%$ & 5 & 10 & 8 \\ \hline
%     \end{tabular}
%     \caption{Window Range Experiment Setup}
%     \label{tab:window_range_experiment_setup}
% \end{center}

% \end{table}

% % TODO talk about results

% \begin{figure}[!ht]
%     \centering
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_2/acra_sample_range}
%         \caption{\gls{swr} for acra}
%         \label{fig:test_2_acra}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_2/dagger_sample_range}
%         \caption{\gls{swr} for dagger}
%         \label{fig:test_2_dagger}
%     \end{minipage}
% \end{figure}

% \begin{figure}[!ht]
%     \centering

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_2/fresco_sample_range}
%         \caption{\gls{swr} for fresco}
%         \label{fig:test_2_fresco}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_2/storm_sample_range}
%         \caption{\gls{swr} for storm}
%         \label{fig:test_2_storm}
%     \end{minipage}
% \end{figure}

% \begin{figure}[!ht]
%     \centering
%         \includegraphics[width=0.45\textwidth]{images/svm/test_2/deeplearning4j_sample_range}
%     \caption{\gls{swr} for deeplearning4j}
%     \label{fig:test_2_deeplearning4j}
% \end{figure}
    
\subsection{Feature Set Experiments}
\label{sec:svm_feature_set_experiments}


\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|cc|}
        \hline
        Extended & Over & Under & Sample & Window & \gls{swr} & SVM & \\
        Window & Sampling & Sampling & Rate & Offset &  & C & gamma \\ \hline
        No & No & Yes & $100\%$ & 5 & 90 & 10 & 8 \\ \hline
    \end{tabular}
    \caption{Feature Experiment Setup}
    \label{tab:svm_feature_experiment_setup}
\end{center}

\end{table}

%% TODO create a table to show this information.
% Extend Window: No% Sample by Commit Range: Yes
% Over Sampling: No
% Under Sampling: Yes
% Sample Rate: 100%
% Window Offset: 5
% SVM c: 10
% SVM gamma: 8
% SVM esp: 0.001

%The parameter for this experiment are outlined in \autoref{tab:window_range_experiment_setup}. The major difference between the \gls{svm} and this experiment, \gls{rf}, is the parameters used for the \gls{rf}. This allows for a fairly clear comparison between these two methods with the given independent variable, sample window size.

This experiment uses different sets of candidate feature to test to explore the available features. The remaining variables were kept constant to allow for the candidate feature sets to be viewed in isolation. These constants are provided in \autoref{tab:svm_feature_experiment_setup}. The value of $90$ for \gls{swr} was selected based on the value being in the middle of the range experimented on for the previous experiment. The remaining variables are kept the same as the previous experiment in \autoref{sec:svm_swr_experiment}. 

\begin{table}[ht]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Feature & Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        1 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ \\
        2 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\
        3 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        4 & & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        5 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & & & $\bullet$ \\ \hline
        %6 & $\bullet$ & & & & $\bullet$ & $\bullet$ & $\bullet$ & \\ 
    \end{tabular}
    \caption{Candidate Feature Sets}
    \label{tab:svm_feature_experiment_sets}
\end{center}
\end{table}

The candidate feature sets are outlined in \autoref{tab:svm_feature_experiment_sets}. Each set is assigned an index value to allow for easier reference later on. For the remainder of this section the experiment sets will be referenced using the assigned index. Therefore if feature set 3 is referenced then that refers to the candidate feature set in the third row.

% \begin{figure}[h]
%     \centering

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_3/acra_sample_range}
%         \caption{Feature for acra using \gls{svm}}
%         \label{fig:test_3_acra_svm}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_3/dagger_sample_range}
%         \caption{Feature for dagger using \gls{svm}}
%         \label{fig:test_3_dagger_svm}
%     \end{minipage}
% \end{figure}

\begin{figure}[!t]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_3/acra_sample_range}
        \caption{Feature for acra using \gls{svm}}
        \label{fig:test_3_acra_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/dagger_sample_range}
        \caption{Feature for dagger using \gls{svm}}
        \label{fig:test_3_dagger_svm}
\end{figure}

% \begin{figure}[h]
%     \centering

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_3/fresco_sample_range}
%         \caption{Feature for fresco using \gls{svm}}
%         \label{fig:test_3_fresco_svm}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_3/storm_sample_range}
%         \caption{Feature for storm using \gls{svm}}
%         \label{fig:test_3_storm_svm}
%     \end{minipage}
% \end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/fresco_sample_range}
        \caption{Feature for fresco using \gls{svm}}
        \label{fig:test_3_fresco_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/storm_sample_range}
        \caption{Feature for storm using \gls{svm}}
        \label{fig:test_3_storm_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_3/deeplearning4j_sample_range}
    \caption{Feature for deeplearning4j using \gls{svm}}
    \label{fig:test_3_deeplearning4j_svm}
\end{figure}

The first project, acra, performs well with accuracy and precision maintaining around $0.7$ with the recall above in \autoref{fig:test_3_acra_svm}. Similarly the recall is always above $0.75$ for each tests even for the feature set that performed the worst. However the rest of the experiments did not perform as well. The accuracy and precision did not very greatly between the different feature sets. The project dagger in \autoref{fig:test_3_dagger_svm} has mixed results with accuracy and precision staying around $0.5$ and recall either very high or low. Likewise for fresco and storm in \autoref{fig:test_3_fresco_svm} and \autoref{fig:test_3_storm_svm} respectively have very little variations in the accuracy and precision. Some of the feature sets tested provide high recall while others provide low without a clear pattern. Finally, deeplearning4j in \autoref{fig:test_3_deeplearning4j_svm} rather similar to results as last two, however unlike the other projects candidate feature set 4 performs the best out of all the feature sets.

\subsection{SVM Oversampling Experiment}
\label{sec:svm_os_experiment}

\gls{os} is a balancing technique used to increase the amount of samples available. Samples from the smaller data set are re-sampled to increase the size of the data set. While this does introduce duplicates into the model it also counter acts biasing that is present when one classification is more common then the other by a large margin. Under sampling is also used to remove excess elements from the larger set of classification. \gls{os} This is especially useful for data sets that contain a small number of samples for a particular category. In that case under sampling may limit the performance of a model by removing nearly all of the elements in the data set.

\begin{table}[ht]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Project & \multicolumn{1}{c}{\gls{swr}} & \\
         & \multicolumn{1}{c}{Best} & \multicolumn{1}{c|}{Worst} \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        acra & 90 & 130 \\
        dagger & 60 & 70 \\
        fresco & 130 & 90 \\
        storm & 80 & 130 \\
        deeplearning4j & 120 & 130 \\ \hline
    \end{tabular}
    \caption{Best And Worst Results From SWR experiments for SVM}
    \label{tab:svm_best_worst_swr_experiment_sets}
\end{center}
\end{table}

The experiment below took the best and worst trials from \autoref{sec:svm_swr_experiment} and used \gls{os} when sampling the data. For each of these experiments the candidate feature set from \autoref{tab:svm_window_range_experiment_features} was used. In some cases the best performing experiment may not have been entirely clear. For example with some projects having very high recall ($ \geq 0.9$) while having lower precision and accuracy. The best trail was picked on having the all three parameters higher rather than one an of the values being outlier. Of course accuracy and precision are very closely related and therefore all three were attempted to be maximized rather than just two of the three.

\begin{figure}[!t]
    \centering

        \includegraphics[width=0.8\textwidth]{images/svm/test_4/acra_sample_range}
        \caption{Oversampling for acra using \gls{svm}}
        \label{fig:test_4_acra_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/dagger_sample_range}
        \caption{Oversampling for dagger using \gls{svm}}
        \label{fig:test_4_dagger_svm}
\end{figure}

% \begin{figure}[h]
%     \centering

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_3/fresco_sample_range}
%         \caption{Feature for fresco using \gls{svm}}
%         \label{fig:test_3_fresco_svm}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/svm/test_3/storm_sample_range}
%         \caption{Feature for storm using \gls{svm}}
%         \label{fig:test_3_storm_svm}
%     \end{minipage}
% \end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/fresco_sample_range}
        \caption{Oversampling for fresco using \gls{svm}}
        \label{fig:test_4_fresco_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/storm_sample_range}
        \caption{Oversampling for storm using \gls{svm}}
        \label{fig:test_4_storm_svm}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/svm/test_4/deeplearning4j_sample_range}
    \caption{Oversampling for deeplearning4j using \gls{svm}}
    \label{fig:test_4_deeplearning4j_svm}
\end{figure}

The best and worst from \autoref{sec:svm_swr_experiment} to determine the effect of \gls{os}. In the plots for this section the performance of the \gls{svm} model from the experiment in \autoref{sec:svm_swr_experiment} is represented by either \textit{best} or \textit{worst}. Alternatively, the values for the performance of the \gls{svm} model when using \gls{os} is represented by either \textit{best-O} or \textit{worst-O}. In \autoref{tab:svm_best_worst_swr_experiment_sets}, the value of \gls{swr} used for to obtain best and worst. For the experiments the \gls{swr} will be set to the corresponding value from \autoref{tab:svm_best_worst_swr_experiment_sets}. Therefore the project \textit{acra}, the trail for \textit{best} and \textit{best-O} both use an \gls{swr} of $90$. Furthermore, for \textit{worst} and \textit{worst-O} will use an \gls{swr} of $130$.

The majority of the experiments there is little to no impact whether \gls{os} was used or not. The only experiment that \gls{os} had a noticeable impact was deeplearning4j. In \autoref{fig:test_4_deeplearning4j_svm} the performance of both the best and the worst with \gls{os} is worse with the exception of a slight improvement for the recall in the best trials.

\subsection{SVM Discussion}
\label{subsec:svm_discussion}

% TODO expand here a little more similar to that of rf discussions.
The three different experiments attempted to determine the impact of the different factors on the prediction method. The three factors that were tested are:
\begin{enumerate}
\item \gls{swr}
\item Model features
\item Sampling balancing
\end{enumerate}
Overall \gls{swr} had the greatest impact on the performance of the prediction method. While the \gls{swr} does have a large impact some projects are less affected. Specifically, the accuracy and precision remained rather stable will primarily small variations for any change in \gls{swr}, model features and sampling balancing. The model's recall however, tended to vary more greatly for the first two experiments. Small changes to the \gls{swr} will result in large changes to the recall. Likewise in some of the trails from the second experiment the recall will change drastically with slightly different sets of features. While positive results were achieved negative results also were present. Furthermore, no clear pattern was discovered to allow for simple configuration of the parameters to provide positive results. Therefore use of this approach with a \gls{svm} model can be beneficial but also incurs a risk associated with poor predictions.   

\section{Random Forest Experiments}
\label{sec:rf_experiments}

The machine learning algorithm \gls{rf} is used for the second set of experiments. \gls{rf} was selected as an alternative to \gls{svm} for it's success in various data mining related tools. The implementation of \gls{rf} is in a python library \textit{scikit-learn} which is outlined in \autoref{sec:prediction_method}. Only one parameter is used for \gls{rf}, the forest size, which is set to $10000$ all of these experiments.

\subsection{Window Range Experiments}
\label{sec:rf_swr_experiment}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Features}
    \label{tab:rf_window_range_experiment_features}
\end{center}

\end{table}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Extended & Over & Under & Sample & Window & \gls{rf} \\
        Window & Sampling & Sampling & Rate & Offset & Size \\ \hline
        No & No & Yes & $100\%$ & 5 & 10000 \\ \hline
    \end{tabular}
    \caption{\gls{swr} Experiment Setup}
    \label{tab:rf_window_range_experiment_setup}
\end{center}

\end{table}

The independent variable for this set of experiments is the sample window size measured in commits. The candidate features are outlined in \autoref{tab:rf_window_range_experiment_features}. The features used for this experiment is the same as the first \gls{svm} experiment the candidate feature set.

% \begin{figure}[h]
%     \centering

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/rf/test_1/acra_sample_range}
%         \caption{\gls{swr} for acra using \gls{rf}}
%         \label{fig:test_1_acra_rf}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/rf/test_1/dagger_sample_range}
%         \caption{\gls{swr} for dagger using \gls{rf}}
%         \label{fig:test_1_dagger_rf}
%     \end{minipage}
% \end{figure}

\begin{figure}[!t]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/acra_sample_range}
        \caption{\gls{swr} for acra using \gls{rf}}
        \label{fig:test_1_acra_rf}
\end{figure}

The parameter for this experiment are outlined in \autoref{tab:rf_window_range_experiment_setup}. The major difference between the \gls{svm} and this experiment, \gls{rf}, is the parameters used for the \gls{rf}. This allows for a fairly clear comparison between these two methods with the given independent variable, the \gls{swr}.

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/dagger_sample_range}
        \caption{\gls{swr} for dagger using \gls{rf}}
        \label{fig:test_1_dagger_rf}
\end{figure}


\begin{figure}[!ht]
    \centering

        \includegraphics[width=0.8\textwidth]{images/rf/test_1/fresco_sample_range}
        \caption{\gls{swr} for fresco using \gls{rf}}
        \label{fig:test_1_fresco_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/storm_sample_range}
        \caption{\gls{swr} for storm using \gls{rf}}
        \label{fig:test_1_storm_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_1/deeplearning4j_sample_range}
    \caption{\gls{swr} for deeplearning4j using \gls{rf}}
    \label{fig:test_1_deeplearning4j_rf}
\end{figure}

The results for the experiments with the independent variable sample window size using random forest were mixed. Both acra in \autoref{fig:test_1_acra_rf} and dagger in \autoref{fig:test_1_dagger_rf} have strong prediction results. While the remainder of the projects; fresco in \autoref{fig:test_1_fresco_rf}, storm in \autoref{fig:test_1_storm_rf} and deeplearning4j in \autoref{fig:test_1_deeplearning4j_rf} did not perform as well. For these projects the accuracy and precision were lower, the recall was very high.

While acra had positive results several values of \gls{swr}, as the \gls{swr} increased the recall decreased as well. Both acra and dagger also had a larger variation in performance for accuracy and precision than the remaining three projects. The actual difference however was smaller with the largest variation of around $0.2$. The lack of a large variation for different values of \gls{swr} points to the independent variable not having as strong of impact on the performance. The variable is still useful to help provide a better result but is not the main contributer.

\subsection{Feature Set Experiments}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        Extended & Over & Under & Sample & Window & \gls{swr} & \gls{rf} \\
        Window & Sampling & Sampling & Rate & Offset &  & Size \\ \hline
        No & No & Yes & $100\%$ & 5 & 90 & 10000 \\ \hline
    \end{tabular}
    \caption{Candidate Feature Experiment Setup}
    \label{tab:rf_feature_experiment_setup}
\end{center}

\end{table}

%The parameter for this experiment are outlined in \autoref{tab:window_range_experiment_setup}. The major difference between the \gls{svm} and this experiment, \gls{rf}, is the parameters used for the \gls{rf}. This allows for a fairly clear comparison between these two methods with the given independent variable, sample window size.

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Feature & Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        1 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ \\
        2 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\
        3 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        4 & & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & & $\bullet$ \\
        5 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & & & $\bullet$ \\ \hline
        %6 & $\bullet$ & & & & $\bullet$ & $\bullet$ & $\bullet$ & \\ 
    \end{tabular}
    \caption{Candidate Feature Sets}
    \label{tab:rf_feature_experiment_sets}
\end{center}

\end{table}

Similar to the experiment using a \gls{svm} in \autoref{sec:svm_feature_set_experiments}. The experiment parameters are outlined in \autoref{tab:rf_feature_experiment_setup}. The candidate features are likewise outlined in \autoref{tab:rf_feature_experiment_sets}. Each set is assigned an index value to allow for easier reference later on in this section. The candidate feature set will be referenced by the index assigned in the plots and discussions related. The candidate feature sets were used experimented on with each project which are discussed below.

\begin{figure}[!t]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/acra_sample_range}
        \caption{Feature for acra using \gls{rf}}
        \label{fig:test_3_acra_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/dagger_sample_range}
        \caption{Feature for dagger using \gls{rf}}
        \label{fig:test_3_dagger_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/fresco_sample_range}
        \caption{Feature for fresco using \gls{rf}}
        \label{fig:test_3_fresco_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/storm_sample_range}
        \caption{Feature for storm using \gls{rf}}
        \label{fig:test_3_storm_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_3/deeplearning4j_sample_range}
    \caption{Feature for deeplearning4j using \gls{rf}}
    \label{fig:test_3_deeplearning4j_rf}
\end{figure}

Generally the experiment results were not particularly strong. In \autoref{fig:test_3_acra_rf} the performance for candidate feature sets 1, 2, 3, 5 were all high and is the performs the best out of all the other projects. For dagger in \autoref{fig:test_3_dagger_rf} the result for candidate feature set 1 is high where as the rest are far worse. The remainder of the projects the accuracy and precision are less than $0.6$. In \autoref{fig:test_3_fresco_rf} and \autoref{fig:test_3_storm_rf} the recall is far more consistently high. However deeplearning4j has generally low results for accuracy, precision and recall.

\subsection{Oversampling Experiment}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        Com & Sig & Name & $f_{\Delta}$ & $sf_{\Delta}$ & $t_\Delta$ & Length & $change_{t-1}$ \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & & $\bullet$ & $\bullet$ & $\bullet$ \\ \hline
    \end{tabular}
    \caption{\gls{os} Experiment Features}
    \label{tab:rf_oversampling_features}
\end{center}

\end{table}

\begin{table}[h]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Extended & Under & Sample & Window & \gls{rf} \\
        Window & Sampling & Rate & Offset & Size \\ \hline
        No & Yes & $100\%$ & 5 & 10000 \\ \hline
    \end{tabular}
    \caption{\gls{os} Experiment Setup}
    \label{tab:rf_os_experiment_setup}
\end{center}

\end{table}


This experiment builds on top of the previous experiment in \autoref{sec:svm_swr_experiment} and shares a very similar setup as the previous experiment. The features used for this experiment are outlined in \autoref{tab:rf_oversampling_features}. The experiment parameters are outlined in \autoref{tab:rf_os_experiment_setup}. The best and worst trails for each project are taken from the previous experiment. The value of the \gls{swr} was recorded in \autoref{tab:rf_best_worst_swr_experiment_sets} for each project for the best and worst performance of the \gls{rf} model. The experiment uses these trails that were preformed without \gls{os} to compare against new trials that use the exact same parameters except for the use of \gls{os}.

\begin{table}[ht]
\begin{center}

    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Project & \multicolumn{1}{c}{\gls{swr}} & \\
         & \multicolumn{1}{c}{Best} & \multicolumn{1}{c|}{Worst} \\
        %Set & & & & & $\Delta_{freq}$ & & & Next \\
         \hline
        acra & 90 & 130 \\
        dagger & 90 & 80 \\
        fresco & 60 & 70 \\
        storm & 60 & 70 \\
        deeplearning4j & 100 & 80 \\ \hline
    \end{tabular}
    \caption{Best And Worst Results From SWR Experiments for RF}
    \label{tab:rf_best_worst_swr_experiment_sets}
\end{center}
\end{table}

The result of a trail with \gls{os} are represented in the figures by either \textit{best-O} or \textit{worst-O}. The results without \gls{os} from the previous experiment are represented with \textit{best} and \textit{worst}.

\begin{figure}[!t]
    \centering

        \includegraphics[width=0.8\textwidth]{images/rf/test_4/acra_sample_range}
        \caption{Oversampling for acra using \gls{rf}}
        \label{fig:test_4_acra_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/dagger_sample_range}
        \caption{Oversampling for dagger using \gls{rf}}
        \label{fig:test_4_dagger_rf}
\end{figure}

% \begin{figure}[h]
%     \centering

%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/rf/test_3/fresco_sample_range}
%         \caption{Feature for fresco using \gls{rf}}
%         \label{fig:test_3_fresco_rf}
%     \end{minipage}
% \quad
%     \begin{minipage}[b]{0.45\linewidth}
%         \includegraphics[width=1.0\textwidth]{images/rf/test_3/storm_sample_range}
%         \caption{Feature for storm using \gls{rf}}
%         \label{fig:test_3_storm_rf}
%     \end{minipage}
% \end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/fresco_sample_range}
        \caption{Oversampling for fresco using \gls{rf}}
        \label{fig:test_4_fresco_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/storm_sample_range}
        \caption{Oversampling for storm using \gls{rf}}
        \label{fig:test_4_storm_rf}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.8\textwidth]{images/rf/test_4/deeplearning4j_sample_range}
    \caption{Oversampling for deeplearning4j using \gls{rf}}
    \label{fig:test_4_deeplearning4j_rf}
\end{figure}

The results for acra showed a slight improvement for both the best and worst case when using \gls{os} in \autoref{fig:test_4_acra_rf}. While the improvements were marginal they are still present nonetheless. Similarly, fresco in \autoref{fig:test_4_fresco_rf} also had a slight improvement in the performance for all three parameters for both \textit{best-O} and \textit{worst-O}.

While both acra and fresco had slight improvements, dagger in \autoref{fig:test_4_dagger_rf} and storm in \autoref{fig:test_4_storm_rf} both had decreases in performance when \gls{os} was employed. The decrease in performance for the precision, accuracy and recall was small. Storm did have a slight improvement in the recall for \textit{worst-O} over \textit{worst}. The final project, deeplearning4j, had mixed results with the use of \gls{os}. In \autoref{fig:test_4_deeplearning4j_rf}, the performance for \textit{best-O} was worse than the original experiment. However the performance of the \textit{worst-O} was marginally better than the original.

\subsection{Random Forest Discussion}
\label{subsec:rf_discussion}

The approach was experimented on using the machine learning algorithm \gls{rf}. The three factors; \gls{swr}, feature set and \gls{os} were investigated. The first factor, \gls{swr}, had positive results for the two smaller projects and lower performance for the remaining projects. The changes made to the \gls{swr} caused little variation between samples. Of course there were some exceptions, for example the recall for acra decreased at $80$, $100$ and $120$. Each drop was around $0.2$ and in total led to a fall of $0.8$. This should however not lessen the positive performance achieved especially in projects like acra.

The second experiment also had very small variability for different values of the independent variable. This time however the independent variable was the feature set used to train and predict with the \gls{rf} model. The accuracy and precision for each project's trial set experienced very small changes. The only two projects to experience a higher amount of variability for the accuracy and precision were acra and dagger. For acra the third feature set decreased the performance. For dagger the first, third and fifth all decreased the performance to around the same level with the second and fourth both providing better performance at around the same level as well. The recall remained high for all projects except for dagger and deeplearning4j. In dagger the recall for the fourth feature set is far lower than the other feature sets at $0.45$. Finally, deeplearning4j for the first feature set has a recall of around $0.5$ and for the fifth feature set around $0.55$. The different feature sets lead to small variations in performance for the \gls{rf}. No clear relationship was determined however, since each project will perform different for given feature sets. For example, acra performs the best with feature set 4. Alternatively, dagger performs best with feature set 2 (which acra also performed well with). For the remaining 3 projects very variation occurs between performance for the 5 feature sets. Another factor of interest is that both acra and dagger had performed well for in the initial experiment using an \gls{swr} of 90. The other three projects which did not perform as well in this experiment also did not perform well at the given \gls{swr} value of 90.

Finally for the third experiment the factor of \gls{os} was investigated. The best and the worst results were taken from the first experiment. \gls{os} was applied to using the same parameters as before. While some projects saw small improvements with the use of \gls{os}, others saw the performance reduce or stay the same. Overall this experiment shows that \gls{os} provides little impact that is inconsistent per project. Since the any increase or decrease to the performance were marginal the impact of \gls{os} on the project is small. While use of \gls{os} for some projects provided an improvement, others the use of \gls{os} was a detriment. The The reason for this could have to do with \gls{rf} being a fairly robust to biasing. However Since the original experiment was already balanced using under sampling the experiment had less to do with the balance of the data set and more to do with the number of samples used. As noted earlier, when both \gls{os} and undersampling are used together then the number of samples re-sampled from the smaller category will be at most double the original amount.

\section{Threats to Validity}
\label{sec:threat_validity}

This wider experimentation also proved to be very beneficial for the analysis of the method since performance was not consistent across all projects. A concerted effort was made to contrasting positive results for one project with negative results. Such a contrast may mitigate the impact of the positive results, however provide the full context and help direct future work in this area.

Each experiment was designed to attempt to provide a robust setup to measure accurately the performance of the approach given the changes to the current factor. The setup was designed to attempt to preventing the influence of other variables beyond the independent variable. The factors that may have had an influence on the experimental results are the third experiment only sampling the extremes (best and worst).

A major concern with the final experiment was that of the sampling of the best and worst results from the previous experiments to test the use of \gls{os}. While the results of the use of \gls{os} should not be discounted, only the sampling the extremes of the previous experiment may have limited the measurable impact of the use of \gls{os}. This experiment could be extended to test the middle performance or even test each trail from the previous experiments.

The differences between the projects prevented a more direct comparison between the projects. Furthermore, as shown through the experiments, certain projects (acra, dagger) generally performed better than other projects (fresco, storm, deeplearning4j). This leads to the conclusion that certain project related factors have a large impact on the performance of the approach. Further investigation into these project specific factors could lead to improved results for the approach.